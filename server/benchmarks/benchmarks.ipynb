{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks of Strategies for Selecting Outdated Items\n",
    "This notebook contains the benchmarks related to the selection strategies for context data, which we report in our paper.\n",
    "Context data are selected from the processed data and included in the next progressive computation step, such that its result approximates that of a _non-progressive_ computation over the processed data.\n",
    "\n",
    "## Benchmark Configuration\n",
    "\n",
    "We use the following configuration in our benchmarks:\n",
    "### Test cases \n",
    "- full computation over the entire dataset (upper baseline)\n",
    "- progressive computation without optimization (lower baseline)\n",
    "- full computation of processed data\n",
    "- progressive computation using optimization strategies\n",
    "\n",
    "### Dataset\n",
    "- NYC taxis dataset (10 Million items), stored in a compressed CSV file, loaded with DuckDB \n",
    "\n",
    "### Variables\n",
    "- dependent variables: runtime, prediction error\n",
    "- independent variables: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an appropriate `chunk size`\n",
    "The number of items in each chunk dictate the computation time for each chunk in the data, in that the more items we process, the longer the DOI computation takes.\n",
    "Therefore, the first consideration in our benchmarks is to find the maximum number of items, for which the computation time remains immediate.\n",
    "Prior work (see Card et al., 1991) has shown this limit to be about one second.\n",
    "\n",
    "In the cell below, we try different chunk sizes to find the maximum items we can pass to the doi function for computations under 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from copy import copy\n",
    "from notebook_test_case import DATA, DOI_CONFIG, PARAMETERS, PATH\n",
    "from context_item_dropdown_strategy.no_context import NoContext\n",
    "from outdated_item_dropdown_strategy.no_update import NoUpdate\n",
    "from storage_strategy.no_storage import NoStorage\n",
    "from test_case import create_test_case, StrategiesConfiguration\n",
    "\n",
    "chunk_sizes = [10, 100, 1000, 10000]\n",
    "\n",
    "for size in chunk_sizes:\n",
    "  before = time()\n",
    "\n",
    "  strategies_config = StrategiesConfiguration(\n",
    "    \"__chunk_size__\",\n",
    "    NoContext(DATA.n_dims, None), \n",
    "    NoUpdate(DATA.n_dims, None), \n",
    "    NoStorage(),\n",
    "  )\n",
    "\n",
    "  parameter_config = copy(PARAMETERS)\n",
    "  parameter_config.chunks = 1\n",
    "  parameter_config.chunk_size = size\n",
    "  \n",
    "  create_test_case(\n",
    "    name=\"__test__\", \n",
    "    strategies=strategies_config,\n",
    "    data=DATA,\n",
    "    doi=DOI_CONFIG,\n",
    "    params=parameter_config,\n",
    "    path=PATH,\n",
    "  ).run()\n",
    "\n",
    "  print(f\"{size}: {time() - before}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Baselines\n",
    "#### Baseline1: Monolithic computation\n",
    "The ground truth for our strategies is a full computation over the entire dataset without any chunking.\n",
    "This computation naturally takes a long time to complete, which is why the progressive scenario is so much more effective from a user perspective: we get to see the data much faster.\n",
    "\n",
    "In the context of the `BenchmarkTestCase` class, the monolithic computation corresponds with running a progression with a single chunk. \n",
    "\n",
    "#### Baseline2: Bigger chunks\n",
    "In addition to the full computation, another important baseline is to compare ourselves against a computation that does not use any strategies, but instead uses the entire `chunk time` to compute a whole new chunk.\n",
    "The idea here is to compare, whether all the context- and outdated-computations are actually valuable, or whether we could just use all resources on processing new data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, Dropdown, Layout, VBox, Output\n",
    "from benchmarks import MODES, get_all_test_cases, run_test_case_ground_truth, run_test_case_bigger_chunks\n",
    "\n",
    "test_cases = get_all_test_cases()\n",
    "\n",
    "presets = list(range(len(test_cases)))\n",
    "modes = MODES\n",
    "\n",
    "test_case_dropdown = Dropdown(\n",
    "  description=\"Select a test case\",\n",
    "  style={\"description_width\": \"initial\"},\n",
    "  options=presets,\n",
    "  disabled=len(presets) == 0,\n",
    ")\n",
    "mode_dropdown = Dropdown(\n",
    "  description=\"Select a mode\",\n",
    "  style={\"description_width\": \"initial\"},\n",
    "  options=modes,\n",
    "  disabled=len(modes) == 0,\n",
    ")\n",
    "\n",
    "start_button = Button(\n",
    "  description=\"start\",\n",
    "  tooltip=\"launches the benchmark with the selected configuration\",\n",
    "  icon=\"check\",\n",
    "  layout=Layout(margin=\"20px 0\")\n",
    ")\n",
    "\n",
    "output = Output()\n",
    "\n",
    "@output.capture()\n",
    "def start(button):\n",
    "  with output:\n",
    "    test_case_index = int(test_case_dropdown.value)\n",
    "    mode = mode_dropdown.value\n",
    "    print(\"ground truth:\\n###\")\n",
    "    run_test_case_ground_truth(test_case_index, mode)\n",
    "    print(\"\\nbigger chunks:\\n###\")\n",
    "    run_test_case_bigger_chunks(test_case_index, mode)\n",
    "    print(\"done\")\n",
    "\n",
    "start_button.on_click(start)\n",
    "VBox([test_case_dropdown, mode_dropdown, start_button, output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, Dropdown, Layout, VBox, Output, widget_output\n",
    "from IPython.display import display\n",
    "from benchmarks import MODES, get_all_test_cases, run_test_case\n",
    "\n",
    "test_cases = get_all_test_cases()\n",
    "\n",
    "presets = list(range(len(test_cases)))\n",
    "modes = MODES\n",
    "\n",
    "start_button = Button(\n",
    "  description=\"start\",\n",
    "  tooltip=\"launches the benchmark with the selected configuration\",\n",
    "  icon=\"check\",\n",
    "  layout=Layout(margin=\"20px 0\")\n",
    ")\n",
    "\n",
    "output = Output()\n",
    "\n",
    "@output.capture()\n",
    "def start(button):\n",
    "  with output:\n",
    "    test_case_index = int(test_case_dropdown.value)\n",
    "    mode = mode_dropdown.value\n",
    "    run_test_case(test_case_index, mode)\n",
    "    print(\"done\")\n",
    "\n",
    "start_button.on_click(start)\n",
    "\n",
    "try:\n",
    "  ui = VBox([test_case_dropdown, mode_dropdown, start_button, output])\n",
    "  display(ui)\n",
    "except NameError:\n",
    "  print(\"run the benchmarking cell first (no need to press start, though)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import jaccard_score, r2_score\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "def evaluate_test_case(test_case: np.ndarray, ground_truth: np.ndarray):\n",
    "  # score = jaccard_score(test_case, ground_truth, average=\"weighted\")\n",
    "  score = r2_score(ground_truth, test_case)\n",
    "  return score\n",
    "\n",
    "ground_truth = results_full[\"doi\"]\n",
    "context_test_case = results_context[\"doi\"]\n",
    "baseline_test_case = results_chunked[\"doi\"]\n",
    "\n",
    "\n",
    "evaluate_test_case(baseline_test_case, ground_truth), evaluate_test_case(context_test_case, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
