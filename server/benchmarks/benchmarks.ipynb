{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark the new `DoiRegressionModel` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from doi_benchmark import Benchmark\n",
    "\n",
    "\n",
    "def benchmark_max_depth(tested_max_depths: range = range(1, 10)):\n",
    "  '''Measures how varying `max_depth` parameter influences progressive DOI prediction scores, over \n",
    "  a series of chunks (without retraining) --- how stable/overfitted is the tree model.'''\n",
    "  benchmark = Benchmark(max_depths=tested_max_depths)\n",
    "  scores = benchmark.run()\n",
    "  return scores\n",
    "\n",
    "\n",
    "def benchmark_retraining_intervals(tested_intervals: range = range(25)):\n",
    "  '''Measures how varying the update interval influences progressive DOI prediction scores.'''\n",
    "  benchmark = Benchmark(intervals=tested_intervals)\n",
    "  scores = benchmark.run()\n",
    "  return scores\n",
    "\n",
    "\n",
    "def benchmark_chunk_size(tested_sizes: range = range(1000, 110000, 10000)):\n",
    "  '''Measures how varying the chunk size influences the progressive DOI prediction scores (without\n",
    "   retraining, without context)'''\n",
    "  benchmark = Benchmark(chunk_size=tested_sizes)\n",
    "  scores = benchmark.run()\n",
    "  return scores\n",
    "\n",
    "\n",
    "def benchmark_context_size(tested_sizes: range = range(0, 11000, 1000)):\n",
    "  '''Measures how varying the context size influences the progressive DOI prediction scores (without\n",
    "   retraining)'''\n",
    "  benchmark = Benchmark(context_size=tested_sizes)\n",
    "  scores = benchmark.run()\n",
    "  return scores\n",
    "\n",
    "\n",
    "def benchmark_context_strategies(context_strats=[\"stratified\", \"minmax\"]):\n",
    "  benchmark = Benchmark(n_chunks=50, context_size=500, intervals=50, measure_doi_error=True, context_strats=context_strats)\n",
    "  scores = benchmark.run()\n",
    "\n",
    "  return scores\n",
    "\n",
    "\n",
    "def benchmark_doi_error_without_context():\n",
    "  '''Measures the changes in DOI error throughout the progression, when no optimizations are \n",
    "   applied.'''\n",
    "  benchmark = Benchmark(n_chunks=100, chunk_size=1000, measure_doi_error=True)\n",
    "  scores = benchmark.run()\n",
    "\n",
    "  return scores\n",
    "\n",
    "  \n",
    "def benchmark_doi_error_with_context():\n",
    "  '''Measures the changes in DOI error throughout the progression, when context optimizations are \n",
    "   applied.'''\n",
    "  benchmark = Benchmark(n_chunks=100, chunk_size=1000, measure_doi_error=True, context_size=1000, intervals=5, context_strats=\"stratified\")\n",
    "  scores = benchmark.run()\n",
    "\n",
    "  return scores\n",
    "\n",
    "def benchmark_retraining_strategies():\n",
    "  '''Measures the changes in DOI error throughout the progression, when context is applied by simply predicting the DOI'''\n",
    "\n",
    "  benchmark = Benchmark(n_chunks=10, chunk_size=1000, context_size=1000, intervals=5, measure_doi_error=True, measure_timings=True, include_previous_chunks_in_training=[True, False])\n",
    "  scores = benchmark.run()\n",
    "\n",
    "  return scores, benchmark\n",
    "\n",
    "def benchmark_with_without_model():\n",
    "  '''Measures the impact of using vs. not using the regression tree model for computing the user interest.'''\n",
    "  benchmark = Benchmark(measure_doi_error=True)\n",
    "\n",
    "  scores = benchmark.run()\n",
    "  return scores, benchmark\n",
    "\n",
    "# TODO: show reduction in overall error when updating \"outdated\" values with each new model\n",
    "\n",
    "# results: max_depth = 3 has best peformance, then no improvement/worse for bigger values\n",
    "# max_depth_results = benchmark_max_depth()\n",
    "\n",
    "# results: trivial best results for 0 and 1, score std for 3-8 above 0.9 \n",
    "# intervals_results = benchmark_retraining_intervals()\n",
    "\n",
    "# results: the larger the context the worse the scores.\n",
    "# context_size_results = benchmark_context_size()\n",
    "\n",
    "# results: the larger the chunk size, the better the scores\n",
    "# chunk_size_results = benchmark_chunk_size()\n",
    "\n",
    "# results: minmax worse than stratified sampling\n",
    "# context_strategies_results = benchmark_context_strategies()\n",
    "\n",
    "# results: overall error increases with every chunk -> DOI values \"age\"\n",
    "# doi_error_without_context_results = benchmark_doi_error_without_context()\n",
    "\n",
    "# results: error increases over time but not as badly as without the context and it gets better\n",
    "# doi_error_with_context_results = benchmark_doi_error_with_context()\n",
    "\n",
    "# results: including the prior model in the update \n",
    "# doi_error_with_prediction_context = benchmark_retraining_strategies()\n",
    "\n",
    "with_without_model = benchmark_with_without_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Computing the DOI function over the entire dataset (sample of 1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monolithic DOI computation: 447.9758746623993s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import os\n",
    "from sys import path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from doi_benchmark import *\n",
    "\n",
    "\n",
    "storage = WindowingStorage(max_size=10000000)\n",
    "\n",
    "reset(intervals=INTERVALS, weights=WEIGHTS)\n",
    "before = time()\n",
    "chunk_df = get_next_progressive_result(storage, chunk_size=1000000, chunk_no=0)\n",
    "print(f\"monolithic DOI computation: {time() - before}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Computing DOI function progressively in chunks without our strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk-based DOI computation: 103.14990496635437s\n",
      "median time per chunk: 0.9594231843948364s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import os\n",
    "from sys import path\n",
    "import numpy as np\n",
    "\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from doi_benchmark import *\n",
    "\n",
    "\n",
    "N = 1000000\n",
    "CHUNK_SIZE = 1000\n",
    "CONTEXT_SIZE = 0\n",
    "N_CHUNKS = N // CHUNK_SIZE\n",
    "\n",
    "storage = WindowingStorage(max_size=N)\n",
    "\n",
    "reset(intervals=INTERVALS, weights=WEIGHTS)\n",
    "timings = []\n",
    "\n",
    "before_all = time()\n",
    "for i in range(100):\n",
    "  before_chunk = time()\n",
    "  chunk_df, new_dois = get_next_progressive_result(\n",
    "      storage,\n",
    "      chunk_size=CHUNK_SIZE,\n",
    "      chunk_no=i,\n",
    "  )\n",
    "  timings += [time() - before_chunk]\n",
    "\n",
    "print(f\"chunk-based DOI computation: {time() - before_all}s\")\n",
    "print(f\"median time per chunk: {np.median(timings)}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3: Computing the DOI function progressively in chunks with our strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk-based DOI computation: 155.70150876045227s\n",
      "median time per chunk: 1.5848214626312256s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import os\n",
    "from sys import path\n",
    "import numpy as np\n",
    "\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from doi_benchmark import *\n",
    "\n",
    "\n",
    "N = 1000000\n",
    "CHUNK_SIZE = 1000\n",
    "CONTEXT_SIZE = 1000\n",
    "N_CHUNKS = N // CHUNK_SIZE\n",
    "\n",
    "storage = WindowingStorage(max_size=N)\n",
    "\n",
    "model = DoiRegressionModel(\n",
    "    storage,\n",
    "    max_depth=4,\n",
    "    include_previous_chunks_in_training=False\n",
    ")\n",
    "\n",
    "reset(intervals=INTERVALS, weights=WEIGHTS)\n",
    "timings = []\n",
    "\n",
    "before_all = time()\n",
    "for i in range(100):\n",
    "  before_chunk = time()\n",
    "  # if no model exists, provide no context...\n",
    "  if i == 0:\n",
    "      chunk_df, new_dois = get_next_progressive_result(\n",
    "          storage,\n",
    "          chunk_size=CHUNK_SIZE,\n",
    "          chunk_no=i,\n",
    "      )\n",
    "      model.update(chunk_df, new_dois)\n",
    "  # ... otherwise use model to provide context\n",
    "  else:\n",
    "      chunk_df, new_dois = get_next_progressive_result(\n",
    "          storage,\n",
    "          chunk_size=CHUNK_SIZE,\n",
    "          chunk_no=i,\n",
    "          get_context=lambda: model.get_context_items(CONTEXT_SIZE),\n",
    "      )\n",
    "  model.update(chunk_df, new_dois, update_outdated=True)\n",
    "  timings += [time() - before_chunk]\n",
    "\n",
    "print(f\"chunk-based DOI computation: {time() - before_all}s\")\n",
    "print(f\"median time per chunk: {np.median(timings)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scenario 3: Computing DOI function progressively in chunks without our strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare visualization with and without our model when progressively analyzing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "s = with_without_model[1].storages[0]\n",
    "\n",
    "nrow = 1\n",
    "ncol = 3\n",
    "plot_size = 4\n",
    "fig, axes = plt.subplots(nrow, ncol, figsize=(ncol * plot_size, plot_size), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(ncol):\n",
    "  # chunk_df = s.get_items_for_chunks(chunks=list(range(i + 1)), as_df=True)\n",
    "  chunk_df = s.get_items_for_chunks(chunks=[i], as_df=True)\n",
    "  doi = s.get_doi_for_ids(chunk_df[\"tripID\"].tolist())\n",
    "  chunk_df[\"doi\"] = doi.astype(float)\n",
    "  \n",
    "  chunk_df.plot.scatter(\n",
    "    x=\"trip_distance\", \n",
    "    y=\"total_amount\", \n",
    "    alpha=chunk_df[\"doi\"], \n",
    "    c=chunk_df[\"doi\"], \n",
    "    ax=axes[i], \n",
    "    cmap=\"viridis\"\n",
    "  )\n",
    "  axes[i].set_xticks([])\n",
    "  axes[i].set_yticks([])\n",
    "\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(axes[0], cax=cbar_ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "doi_error_with_prediction_context[~doi_error_with_prediction_context[\"include_previous_chunks_in_training\"]].plot(x=\"chunk\", y=\"error_mean\", ax=axes[0])\n",
    "doi_error_with_prediction_context[doi_error_with_prediction_context[\"include_previous_chunks_in_training\"]].plot(x=\"chunk\", y=\"error_mean\", ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5), sharey=True)\n",
    "\n",
    "doi_error_with_prediction_context.boxplot(column=\"score\", by=\"include_previous_chunks_in_training\", ax=axes[0])\n",
    "\n",
    "doi_error_with_prediction_context[~doi_error_with_prediction_context[\"include_previous_chunks_in_training\"]].plot(y=\"score\", x=\"chunk\", ax=axes[1])\n",
    "\n",
    "doi_error_with_prediction_context[doi_error_with_prediction_context[\"include_previous_chunks_in_training\"]].plot(y=\"score\", x=\"chunk\", ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True, sharex=True)\n",
    "\n",
    "doi_error_without_context_results[\"error_std\"].plot(ax=axes[0])\n",
    "doi_error_with_context_results[\"error_std\"].plot(ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "context_strategies_results.boxplot(column=\"error_std\", by=\"context_strat\", ax=axes[0])\n",
    "context_strategies_results.plot(y=\"error_std\", x=\"chunk\",  ax=axes[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_results.boxplot(column=\"score\", by=\"max_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_results.boxplot(column=\"score\", by=\"interval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size_results.boxplot(column=\"score\", by=\"context_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_results.boxplot(column=\"score\", by=\"chunk_size\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46092ee2eab2a2b22a3e3ce35717d01e098c5874b352c7dbdb7d6cf8b8036d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
