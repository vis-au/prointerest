{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark the new `DoiRegressionModel` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from database import (create_tables, drop_tables, get_next_chunk_from_db,\n",
    "                      reset_progression)\n",
    "from doi_function import (compute_dois, reset_doi_component,\n",
    "                          set_dimension_intervals, set_dimension_weights)\n",
    "from doi_regression_model import *\n",
    "from storage_strategy.windowing_storage import WindowingStorage\n",
    "\n",
    "from server import taxi_process_chunk\n",
    "\n",
    "\n",
    "def reset(weights: dict, intervals: dict): \n",
    "  drop_tables()\n",
    "\n",
    "  create_tables(\n",
    "      row_data_path=\"../data/nyc_taxis.shuffled_full.csv.gz\",\n",
    "      column_data_path=\"../data/nyc_taxis.shuffled_full.parquet\",\n",
    "      id_column=\"tripID\",\n",
    "      total_size=112145904,\n",
    "      process_chunk_callback=taxi_process_chunk,\n",
    "  )\n",
    "\n",
    "  reset_progression()\n",
    "  reset_doi_component()\n",
    "  set_dimension_weights(weights)\n",
    "  set_dimension_intervals(intervals)\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "WEIGHTS = {\n",
    "  \"trip_distance\": 0.25,\n",
    "  \"total_amount\": 0.25,\n",
    "  \"tip_amount\": 0.25,\n",
    "  \"trip_duration\": 0.25,\n",
    "}\n",
    "INTERVALS = {\n",
    "  \"trip_distance\": [16, 20],\n",
    "  \"total_amount\": [34, 74],\n",
    "  \"tip_amount\": [4, 12],\n",
    "  \"trip_duration\": [3, 5],\n",
    "}\n",
    "\n",
    "reset(intervals=INTERVALS, weights=WEIGHTS)\n",
    "\n",
    "\n",
    "def get_next_progressive_result(storage: StorageStrategy, get_context = None):\n",
    "  '''Wrapper function for getting a new chunk, computing the DOI function on it and storing the data\n",
    "     for later retrieval in the storage.'''\n",
    "\n",
    "  # get chunk and compute context\n",
    "  chunk_df = get_next_chunk_from_db(CHUNK_SIZE, as_df=True)\n",
    "  \n",
    "  context_df = pd.DataFrame([], columns = chunk_df.columns)\n",
    "  if get_context is not None:\n",
    "    context_df = get_context()\n",
    "    print(context_df)\n",
    "\n",
    "  # compute the DOI over chunk + context\n",
    "  df = pd.concat([chunk_df, context_df], ignore_index=True)\n",
    "  dois = compute_dois(df)  # HACK: compatibility with DoiComponent class\n",
    "  new_dois = dois[:len(chunk_df)]\n",
    "\n",
    "  storage.insert_chunk(chunk_df, 0)\n",
    "\n",
    "  return chunk_df, new_dois\n",
    "\n",
    "\n",
    "def benchmark_max_depth(tested_max_depths: range = range(1, 10), n_chunks: int = 25):\n",
    "  '''Tests how varying `max_depth` parameter influences progressive DOI prediction scores.'''\n",
    "  scores = []\n",
    "\n",
    "  for test_case_depth in tested_max_depths:\n",
    "    reset(intervals=INTERVALS, weights=WEIGHTS)\n",
    "    storage = WindowingStorage(max_size=10000000)\n",
    "    model = DoiRegressionModel(storage, max_depth=test_case_depth)\n",
    "\n",
    "    test_case_scores_per_chunk = []\n",
    "    for i in range(n_chunks):\n",
    "      chunk_df, new_dois = get_next_progressive_result(storage)\n",
    "      if i == 0:\n",
    "        model.update(chunk_df, new_dois)\n",
    "      test_case_scores_per_chunk += [model.score(chunk_df, new_dois)]\n",
    "    scores += [test_case_scores_per_chunk]\n",
    "  \n",
    "  return pd.DataFrame(np.array(scores).T, columns=tested_max_depths)\n",
    "\n",
    "\n",
    "def benchmark_retraining_intervals(tested_intervals: range = range(25), max_depth: int = 3, n_chunks: int = 25):\n",
    "  '''Tests how varying the update interval influences progressive DOI prediction scores.'''\n",
    "  scores = []\n",
    "\n",
    "  for test_case_interval in tested_intervals:\n",
    "    reset(intervals=INTERVALS, weights=WEIGHTS)\n",
    "    storage = WindowingStorage(max_size=10000000)\n",
    "    model = DoiRegressionModel(storage, max_depth=max_depth)\n",
    "\n",
    "    test_case_scores_per_chunk = []\n",
    "    for i in range(n_chunks):\n",
    "      chunk_df, new_dois = get_next_progressive_result(storage)\n",
    "      if test_case_interval == 0 or i % test_case_interval == 0:\n",
    "        model.update(chunk_df, new_dois)\n",
    "      test_case_scores_per_chunk += [model.score(chunk_df, new_dois)]\n",
    "    scores += [test_case_scores_per_chunk]\n",
    "\n",
    "  return pd.DataFrame(np.array(scores).T, columns=tested_intervals)\n",
    "\n",
    "def benchmark_context(max_depth: int = 3, n_chunks: int = 25):  \n",
    "  scores_with_context = []\n",
    "  context_size = 1000\n",
    "  reset(intervals=INTERVALS, weights=WEIGHTS)\n",
    "  storage = WindowingStorage(max_size=10000000)\n",
    "  model = DoiRegressionModel(storage, max_depth=max_depth)\n",
    "  for i in range(n_chunks):\n",
    "      # model is not yet trained at first chunk, so no context ...\n",
    "      if i == 0:\n",
    "        chunk_df, new_dois = get_next_progressive_result(storage)\n",
    "      # ... otherwise use context\n",
    "      else:\n",
    "        chunk_df, new_dois = get_next_progressive_result(\n",
    "          storage,\n",
    "          lambda: model.get_context_items(context_size)\n",
    "        )\n",
    "\n",
    "      if i == 0:\n",
    "        model.update(chunk_df, new_dois)\n",
    "      scores_with_context += [model.score(chunk_df, new_dois)]\n",
    "\n",
    "  scores_without_context = []\n",
    "  reset(intervals=INTERVALS, weights=WEIGHTS)\n",
    "  storage = WindowingStorage(max_size=10000000)\n",
    "  model = DoiRegressionModel(storage, max_depth=max_depth)\n",
    "  for i in range(n_chunks):\n",
    "      chunk_df, new_dois = get_next_progressive_result(storage)\n",
    "      if i == 0:\n",
    "        model.update(chunk_df, new_dois)\n",
    "      scores_without_context += [model.score(chunk_df, new_dois)]\n",
    "\n",
    "\n",
    "  scores = [scores_with_context, scores_without_context]\n",
    "\n",
    "  return pd.DataFrame(np.array(scores).T, columns=[\"context\", \"no context\"])\n",
    "\n",
    "# results: max_depth = 3 has best peformance, then no improvement/worse for bigger values\n",
    "# max_depth_results = benchmark_max_depth()\n",
    "\n",
    "# results: trivial best results for 0 and 1, score std for 3-8 above 0.9 \n",
    "# intervals_results = benchmark_retraining_intervals()\n",
    "\n",
    "# results: when including context items in the doi computation, scores drop.\n",
    "context_results = benchmark_context()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prediction scores of the regression model across varying `max_depth` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_results.boxplot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46092ee2eab2a2b22a3e3ce35717d01e098c5874b352c7dbdb7d6cf8b8036d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
