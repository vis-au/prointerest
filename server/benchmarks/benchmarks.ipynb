{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks of Strategies for Selecting Outdated Items\n",
    "This notebook contains the benchmarks related to the selection strategies for context data, which we report in our paper.\n",
    "Context data are selected from the processed data and included in the next progressive computation step, such that its result approximates that of a _non-progressive_ computation over the processed data.\n",
    "\n",
    "## Benchmark Configuration\n",
    "\n",
    "We use the following configuration in our benchmarks:\n",
    "### Test cases \n",
    "- full computation over the entire dataset (upper baseline)\n",
    "- progressive computation without optimization (lower baseline)\n",
    "- full computation of processed data\n",
    "- progressive computation using optimization strategies\n",
    "\n",
    "### Dataset\n",
    "- NYC taxis dataset (10 Million items), stored in a compressed CSV file, loaded with DuckDB \n",
    "\n",
    "### Variables\n",
    "- dependent variables: runtime, prediction error\n",
    "- independent variables: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the DOI function\n",
    "In our experiments, we will use the DOI function below, which measures outlierness (i.e., how unique a datum is compared to the rest of the data).\n",
    "Outlierness cannot be expressed by looking at an individual data item, but can only expressed within the context of other data, which is why a purely progressive computation of this DOI measure will inherently be inaccurate compared to a \"monolithic\" computation that looks at all data at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doi_component.outlierness_component import OutliernessComponent\n",
    "\n",
    "outlierness = OutliernessComponent([\"ratio\", \"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from database import initialize_db, drop_tables\n",
    "def reset():\n",
    "  drop_tables()\n",
    "  initialize_db(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an appropriate `chunk size`\n",
    "The number of items in each chunk dictate the computation time for each chunk in the data, in that the more items we process, the longer the DOI computation takes.\n",
    "Therefore, the first consideration in our benchmarks is to find the maximum number of items, for which the computation time remains immediate.\n",
    "Prior work (see Card et al., 1991) has shown this limit to be about one second.\n",
    "\n",
    "In the cell below, we thus compute the maximum number of items, for which the DOI function still returns values after one second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sizes = [10, 100, 1000, 10000]\n",
    "reset()\n",
    "for size in chunk_sizes:\n",
    "  \n",
    "  print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: Chunk-based computation without any optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: no context, update: no chunk\n",
      "done: 197.07497572898865s\n",
      "context: no context, update: oldest n chunks\n",
      "done: 501.54559350013733s\n",
      "context: no context, update: last n chunks\n",
      "done: 1565.0280170440674s\n",
      "context: no context, update: regular intervals\n",
      "done: 510.384170293808s\n",
      "context: no context, update: outdated bins\n",
      "done: 213.77439284324646s\n",
      "context: chunk based, update: no chunk\n",
      "done: 481.0443522930145s\n",
      "context: chunk based, update: oldest n chunks\n",
      "done: 786.4391601085663s\n",
      "context: chunk based, update: last n chunks\n",
      "done: 1954.5901024341583s\n",
      "context: chunk based, update: regular intervals\n",
      "done: 828.6719510555267s\n",
      "context: chunk based, update: outdated bins\n",
      "done: 539.5761635303497s\n",
      "context: sampling based, update: no chunk\n",
      "done: 342.1771938800812s\n",
      "context: sampling based, update: oldest n chunks\n",
      "done: 664.3325853347778s\n",
      "context: sampling based, update: last n chunks\n",
      "done: 1697.6373136043549s\n",
      "context: sampling based, update: regular intervals\n",
      "done: 673.5458197593689s\n",
      "context: sampling based, update: outdated bins\n",
      "done: 332.7411906719208s\n",
      "context: clustering based, update: no chunk\n",
      "done: 8136.847434282303s\n",
      "context: clustering based, update: oldest n chunks\n",
      "done: 51949.37706851959s\n",
      "context: clustering based, update: last n chunks\n",
      "done: 11655.483462572098s\n",
      "context: clustering based, update: regular intervals\n",
      "done: 9108.466406583786s\n",
      "context: clustering based, update: outdated bins\n",
      "done: 8659.209790229797s\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './out/99999/1000/doi/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22136/2519526131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m   \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoi_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\au629923\\Repositories\\prointerest\\server\\benchmarks\\benchmark_test_case.py\u001b[0m in \u001b[0;36mdoi_histogram\u001b[1;34m(self, bins)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoi_csv_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No CSV file for DOI values found. Supply a path during run().\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoi_csv_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDOI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './out/99999/1000/doi/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sys import path\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from database import initialize_db, drop_tables, get_next_chunk_from_db\n",
    "from benchmark_test_case import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from doi_component.outlierness_component import OutliernessComponent\n",
    "outlierness = OutliernessComponent([\"ratio\", \"duration\"])\n",
    "\n",
    "n_dims = 17\n",
    "total_items = 99999\n",
    "chunk_size = 1000\n",
    "chunks = round(total_items / chunk_size)\n",
    "\n",
    "data_path = \"../data/nyc_taxis_sampled100k_shuffled.csv.gz\"\n",
    "\n",
    "def reset():\n",
    "  drop_tables()\n",
    "  initialize_db(data_path)\n",
    "\n",
    "from outdated_item_selection_strategy.no_update import *\n",
    "from outdated_item_selection_strategy.oldest_chunks_update import *\n",
    "from outdated_item_selection_strategy.last_n_chunks_update import *\n",
    "from outdated_item_selection_strategy.regular_interval_update import *\n",
    "from outdated_item_selection_strategy.outdated_bin_update import *\n",
    "\n",
    "update_strategies = [\n",
    "  (\"no chunk\", NoUpdate(n_dims=n_dims)),\n",
    "  (\"oldest n chunks\", OldestChunksUpdate(n_dims=n_dims, n_chunks=3, max_age=10)),\n",
    "  (\"last n chunks\", LastNChunksUpdate(n_dims=n_dims, n_chunks=3)),\n",
    "  (\"regular intervals\", RegularIntervalUpdate(n_dims=n_dims,interval=2, max_age=10)),\n",
    "  (\"outdated bins\", OutdatedBinUpdate(n_dims=n_dims))\n",
    "]\n",
    "\n",
    "from context_item_selection_strategy.no_context import * \n",
    "from context_item_selection_strategy.chunk_based_context import *\n",
    "from context_item_selection_strategy.sampling_based_context import *\n",
    "from context_item_selection_strategy.clustering_based_context import *\n",
    "\n",
    "context_strategies = [\n",
    "  (\"no context\", NoContext(n_dims=n_dims)),\n",
    "  (\"chunk based\", RandomChunkBasedContext(n_dims=n_dims, n_chunks=3)),\n",
    "  (\"sampling based\", RandomSamplingBasedContext(n_dims=n_dims, n_samples=chunk_size)),\n",
    "  (\"clustering based\", ClusteringBasedContext(n_dims=n_dims, n_clusters=chunk_size))\n",
    "]\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "test_cases = []\n",
    "\n",
    "path = f\"./out/{total_items}/{chunk_size}\"\n",
    "if not exists(f\"./out/{total_items}\"):\n",
    "  os.mkdir(f\"./out/{total_items}\")\n",
    "if not exists(path):\n",
    "  os.mkdir(path)\n",
    "\n",
    "\n",
    "for c_strat in context_strategies:\n",
    "  for u_strat in update_strategies:\n",
    "    print(f\"context: {c_strat[0]}, update: {u_strat[0]}\")\n",
    "    \n",
    "    # check if already completed\n",
    "    label = f\"{c_strat[0]}-{u_strat[0]}\"\n",
    "    if os.path.isfile(f\"{path}/doi/{label}.csv\") or os.path.isfile(f\"{path}/times/{label}.csv\"):\n",
    "      print(\"skipping test case because already completed.\")\n",
    "      continue\n",
    "\n",
    "    reset()\n",
    "    test_case = BenchmarkTestCase(label, outlierness, c_strat[1], u_strat[1], chunk_size, chunks)\n",
    "    test_case.run(doi_csv_path=f\"{path}/doi/\", times_csv_path=f\"{path}/times/\")\n",
    "    test_cases += [test_case]\n",
    "    print(f\"done: {test_case.total_time}s\")\n",
    "\n",
    "\n",
    "for tc in test_cases: \n",
    "  tc.doi_histogram()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tc in test_cases: \n",
    "  tc.doi_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 17\n",
    "total_items = 99999\n",
    "chunk_size = 1000\n",
    "chunks = round(total_items / chunk_size)\n",
    "outlierness = OutliernessComponent([\"ratio\", \"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()\n",
    "\n",
    "# upper baseline: full computation over the processed data so far.\n",
    "start = time.time()\n",
    "data = get_next_chunk_from_db(chunk_size * chunks, as_df=True)\n",
    "upper_bound_result = outlierness.compute_doi(data)\n",
    "time_upper = time.time() - start\n",
    "\n",
    "print(f\"# upper bound: {time_upper}\")\n",
    "upper_bound_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import jaccard_score, r2_score\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "def evaluate_test_case(test_case: np.ndarray, ground_truth: np.ndarray):\n",
    "  # score = jaccard_score(test_case, ground_truth, average=\"weighted\")\n",
    "  score = r2_score(ground_truth, test_case)\n",
    "  return score\n",
    "\n",
    "ground_truth = results_full[\"doi\"]\n",
    "context_test_case = results_context[\"doi\"]\n",
    "baseline_test_case = results_chunked[\"doi\"]\n",
    "\n",
    "\n",
    "evaluate_test_case(baseline_test_case, ground_truth), evaluate_test_case(context_test_case, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
