{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks of Strategies for Selecting Outdated Items\n",
    "This notebook contains the benchmarks related to the selection strategies for context data, which we report in our paper.\n",
    "Context data are selected from the processed data and included in the next progressive computation step, such that its result approximates that of a _non-progressive_ computation over the processed data.\n",
    "\n",
    "## Benchmark Configuration\n",
    "\n",
    "We use the following configuration in our benchmarks:\n",
    "### Test cases \n",
    "- full computation over the entire dataset (upper baseline)\n",
    "- progressive computation without optimization (lower baseline)\n",
    "- full computation of processed data\n",
    "- progressive computation using optimization strategies\n",
    "\n",
    "### Dataset\n",
    "- NYC taxis dataset (10 Million items), stored in a compressed CSV file, loaded with DuckDB \n",
    "\n",
    "### Variables\n",
    "- dependent variables: runtime, prediction error\n",
    "- independent variables: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from database import initialize_db, drop_tables\n",
    "from benchmark_test_case import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from doi_component.outlierness_component import OutliernessComponent\n",
    "outlierness = OutliernessComponent([\"ratio\", \"duration\"])\n",
    "\n",
    "total_size = 99999  # total number of processed items, not nec. the size of the data\n",
    "chunk_size = 1000  # number of new items retrieved per step\n",
    "chunks = round(total_size / chunk_size)  # number of steps\n",
    "max_size = chunk_size * 5  # maximum size of storages\n",
    "\n",
    "data_path = \"../data/nyc_taxis.shuffled_full.csv.gz\"\n",
    "column_data_path = \"../data/nyc_taxis.shuffled_full.parquet\"\n",
    "n_dims = 17  # number of dimensions in the data\n",
    "\n",
    "def reset():\n",
    "  drop_tables()\n",
    "  initialize_db(data_path, column_data_path)\n",
    "\n",
    "from storage_strategy.no_storage_strategy import *\n",
    "from storage_strategy.compression_strategy import *\n",
    "from storage_strategy.progressive_bin_sampler import *\n",
    "from storage_strategy.reservoir_sampling_strategy import *\n",
    "from storage_strategy.windowing_strategy import *\n",
    "\n",
    "storage_strategies = [\n",
    "  (\"no_storage_strategy\", NoStorageStrategy()),\n",
    "  (\"compression_strategy\", CompressionStrategy(max_size=max_size)),\n",
    "  (\"progressive_bin_sampler\", ProgressiveBinSampler()),\n",
    "  (\"reservoir_sampling_strategy\", ReservoirSamplingStrategy(max_size=max_size)),\n",
    "  (\"windowing_strateg\", WindowingStrategy(max_size=max_size))\n",
    "]\n",
    "\n",
    "from outdated_item_selection_strategy.no_update import *\n",
    "from outdated_item_selection_strategy.oldest_chunks_update import *\n",
    "from outdated_item_selection_strategy.last_n_chunks_update import *\n",
    "from outdated_item_selection_strategy.regular_interval_update import *\n",
    "from outdated_item_selection_strategy.outdated_bin_update import *\n",
    "\n",
    "update_strategies = [\n",
    "  (\"no chunk\", NoUpdate(n_dims=n_dims, storage=None)),\n",
    "  (\"oldest n chunks\", OldestChunksUpdate(n_dims=n_dims, storage=None, n_chunks=3, max_age=10)),\n",
    "  (\"last n chunks\", LastNChunksUpdate(n_dims=n_dims, storage=None, n_chunks=3)),\n",
    "  (\"regular intervals\", RegularIntervalUpdate(n_dims=n_dims, storage=None, interval=2, max_age=10)),\n",
    "  (\"outdated bins\", OutdatedBinUpdate(n_dims=n_dims, storage=None) )\n",
    "]\n",
    "\n",
    "from context_item_selection_strategy.no_context import * \n",
    "from context_item_selection_strategy.chunk_based_context import *\n",
    "from context_item_selection_strategy.sampling_based_context import *\n",
    "from context_item_selection_strategy.clustering_based_context import *\n",
    "\n",
    "context_strategies = [\n",
    "  (\"no context\", NoContext(n_dims=n_dims, storage=None)),\n",
    "  (\"chunk based\", RandomChunkBasedContext(n_dims=n_dims, n_chunks=3, storage=None)),\n",
    "  (\"sampling based\", RandomSamplingBasedContext(n_dims=n_dims, n_samples=chunk_size, storage=None)),\n",
    "  (\"clustering based\", ClusteringBasedContext(n_dims=n_dims, n_clusters=chunk_size, storage=None))\n",
    "]\n",
    "\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "# create the path for storing the benchmark results if they do not exist\n",
    "path = f\"./out/{total_size}/{chunk_size}\"\n",
    "if not exists (\"./out\"):\n",
    "  os.mkdir(\"./out\")\n",
    "if not exists(f\"./out/{total_size}\"):\n",
    "  os.mkdir(f\"./out/{total_size}\")\n",
    "if not exists(path):\n",
    "  os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an appropriate `chunk size`\n",
    "The number of items in each chunk dictate the computation time for each chunk in the data, in that the more items we process, the longer the DOI computation takes.\n",
    "Therefore, the first consideration in our benchmarks is to find the maximum number of items, for which the computation time remains immediate.\n",
    "Prior work (see Card et al., 1991) has shown this limit to be about one second.\n",
    "\n",
    "In the cell below, we try different chunk sizes to find the maximum items we can pass to the doi function for computations under 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 0.3262362480163574s\n",
      "100: 0.2692584991455078s\n",
      "1000: 0.8617279529571533s\n",
      "10000: 10.674559354782104s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from database import get_from_data\n",
    "\n",
    "chunk_sizes = [10, 100, 1000, 10000]\n",
    "reset()\n",
    "for size in chunk_sizes:\n",
    "  before = time()\n",
    "  data = get_from_data([f\"TRUE LIMIT {size}\"], as_df=True)\n",
    "  data = process_chunk(data)\n",
    "  outlierness.compute_doi(data)\n",
    "  print(f\"{size}: {time() - before}s\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the ground truth: Monolithic computation\n",
    "The ground truth for our strategies is a full computation over the entire dataset without any chunking.\n",
    "This computation naturally takes a long time to complete, which is why the progressive scenario is so much more effective from a user perspective: we get to see the data much faster.\n",
    "\n",
    "In the context of the `BenchmarkTestCase` class, the monolithic computation corresponds with running a progression with a single chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()\n",
    "ground_truth_test_case = BenchmarkTestCase(\n",
    "  name=\"__ground_truth__\", \n",
    "  doi=outlierness, \n",
    "  storage_strategy=NoStorageStrategy(), \n",
    "  context_strategy=NoContext(n_dims, None), \n",
    "  update_strategy=NoUpdate(n_dims, None), \n",
    "  chunk_size=total_size, \n",
    "  chunks=1\n",
    ")\n",
    "ground_truth_test_case.run(doi_csv_path=f\"{path}/doi/\")\n",
    "# 99,999@1000: 20m 48.4s --> 1248s total --> 1.248s per \"chunk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []\n",
    "total = len(context_strategies) * len(update_strategies)\n",
    "i = 0\n",
    "print(f\"items: {total_size}, chunk size: {chunk_size}\\n####\")\n",
    "for c_strat in context_strategies:\n",
    "  for u_strat in update_strategies:\n",
    "    i += 1\n",
    "    print(f\"({i}/{total})context: {c_strat[0]}, update: {u_strat[0]}\")\n",
    "    \n",
    "    # check if already completed\n",
    "    label = f\"{c_strat[0]}-{u_strat[0]}\"\n",
    "    if os.path.isfile(f\"{path}/doi/{label}.csv\") or os.path.isfile(f\"{path}/times/{label}.csv\"):\n",
    "      print(\"skipping test case because already completed.\")\n",
    "      continue\n",
    "\n",
    "    reset()\n",
    "    test_case = BenchmarkTestCase(label, outlierness, WindowingStrategy(max_size), c_strat[1], u_strat[1], chunk_size, chunks)\n",
    "    test_case.run(doi_csv_path=f\"{path}/doi/\", times_csv_path=f\"{path}/times/\")\n",
    "    test_cases += [test_case]\n",
    "    print(f\"done: {test_case.total_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOI histograms per test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "charts = []\n",
    "\n",
    "# load all data from the out directory into one dataframe and add a column that indicates the context\n",
    "# and update strategies used in this particular use case\n",
    "available_test_cases = os.listdir(f\"{path}/doi\")\n",
    "available_test_cases\n",
    "\n",
    "all_doi_values_df = pd.DataFrame()\n",
    "\n",
    "# approach2: manually compute the histogram over all groups in the data, then visualize those bins\n",
    "# histogram = np.histogram(all_doi_values_df[\"doi\"])\n",
    "\n",
    "def get_doi_bins_df(doi_df: pd.DataFrame) -> pd.DataFrame:\n",
    "  bins = np.histogram(doi_df[\"doi\"], bins=10, range=(0, 1))[0]\n",
    "  bins_df = pd.DataFrame(bins.transpose())\n",
    "  return bins_df\n",
    "\n",
    "# compute the ground truth bins \n",
    "ground_truth_df = pd.read_csv(f\"{path}/doi/__ground_truth__.csv\")\n",
    "ground_truth_bins = get_doi_bins_df(ground_truth_df)\n",
    "\n",
    "all_doi_bins_df = pd.DataFrame()\n",
    "\n",
    "# compute the bins for each combination of strategies and then compare it to the ground truth in a \n",
    "# layered histogram\n",
    "for c_strat in context_strategies:\n",
    "  for u_strat in update_strategies:\n",
    "    # check if that test case exists\n",
    "    test_case = f\"{c_strat[0]}-{u_strat[0]}.csv\"\n",
    "    if test_case not in available_test_cases:\n",
    "      continue\n",
    "\n",
    "    # read the benchmark results\n",
    "    df = pd.read_csv(f\"{path}/doi/{test_case}\")\n",
    "\n",
    "    # compute 10 bins on the interval [0, 1] over the \"doi\" column\n",
    "    bins_df = get_doi_bins_df(df)\n",
    "\n",
    "    # compute difference to the ground truth\n",
    "    bins_df[\"delta\"] = bins_df[0] - ground_truth_bins[0]\n",
    "    bins_df.columns = [\"doi\", \"delta\"]\n",
    "\n",
    "    # add context info\n",
    "    bins_df[\"bin\"] = bins_df.index\n",
    "    bins_df[\"context_strategy\"] = c_strat[0]\n",
    "    bins_df[\"update_strategy\"] = u_strat[0]\n",
    "\n",
    "    # store these bins in a df\n",
    "    all_doi_bins_df = all_doi_bins_df.append(bins_df)\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.data_transformers.enable(\"data_server\")\n",
    "alt.Chart(all_doi_bins_df).mark_bar().encode(\n",
    "  x=alt.X(\"bin:N\"),\n",
    "  y=alt.Y(\"delta:Q\"),\n",
    "  color={\"value\": \"red\"}\n",
    ").properties(\n",
    "  width=100,\n",
    "  height=100\n",
    ").facet(\n",
    "  row=\"context_strategy\",\n",
    "  column=\"update_strategy\",\n",
    "  spacing=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series per test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-c39bf861840745e89175442ce21a6573\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c39bf861840745e89175442ce21a6573\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c39bf861840745e89175442ce21a6573\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"url\": \"http://localhost:20020/f7e96ac95e95b77e1d5d89a800f1ba90.json\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"context_strategy\", \"type\": \"nominal\"}, \"column\": {\"field\": \"update_strategy\"}, \"x\": {\"field\": \"chunk\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"total_time\", \"scale\": {\"type\": \"linear\"}, \"title\": \"time (s)\", \"type\": \"quantitative\"}}, \"height\": 500, \"width\": 200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\"}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "\n",
    "charts = []\n",
    "\n",
    "# load all data from the out directory into one dataframe and add a column that indicates the context\n",
    "# and update strategies used in this particular use case\n",
    "available_test_cases = os.listdir(f\"{path}/times\")\n",
    "available_test_cases\n",
    "\n",
    "all_doi_values_df = pd.DataFrame()\n",
    "\n",
    "# build one big dataframe containing all doi scores and label each based on the strategies that were\n",
    "# used to generate them\n",
    "for c_strat in context_strategies:\n",
    "  for u_strat in update_strategies:\n",
    "    # check if that test case exists\n",
    "    test_case = f\"{c_strat[0]}-{u_strat[0]}.csv\"\n",
    "    if test_case not in available_test_cases:\n",
    "      continue\n",
    "\n",
    "    df = pd.read_csv(f\"{path}/times/{test_case}\")\n",
    "    df[\"context_strategy\"] = c_strat[0]\n",
    "    df[\"update_strategy\"] = u_strat[0]\n",
    "    all_doi_values_df = all_doi_values_df.append(df)\n",
    "    all_doi_values_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# approach1: manually compute the histogram over all groups in the data, then visualize those bins\n",
    "# histogram = np.histogram(all_doi_values_df[\"doi\"])\n",
    "# approach2: use altair to do the grouping and binning.\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(all_doi_values_df).mark_line().encode(\n",
    "  x=\"chunk:Q\",\n",
    "  y={\"field\": \"total_time\", \"type\": \"quantitative\", \"scale\": {\"type\": \"linear\"}, \"title\": \"time (s)\"},\n",
    "  column={\"field\": \"update_strategy\"},\n",
    "  color=\"context_strategy:N\",\n",
    ").properties(\n",
    "  width=200,\n",
    "  height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import jaccard_score, r2_score\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "def evaluate_test_case(test_case: np.ndarray, ground_truth: np.ndarray):\n",
    "  # score = jaccard_score(test_case, ground_truth, average=\"weighted\")\n",
    "  score = r2_score(ground_truth, test_case)\n",
    "  return score\n",
    "\n",
    "ground_truth = results_full[\"doi\"]\n",
    "context_test_case = results_context[\"doi\"]\n",
    "baseline_test_case = results_chunked[\"doi\"]\n",
    "\n",
    "\n",
    "evaluate_test_case(baseline_test_case, ground_truth), evaluate_test_case(context_test_case, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
