{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks of Strategies for Selecting Outdated Items\n",
    "This notebook contains the benchmarks related to the selection strategies for context data, which we report in our paper.\n",
    "Context data are selected from the processed data and included in the next progressive computation step, such that its result approximates that of a _non-progressive_ computation over the processed data.\n",
    "\n",
    "## Benchmark Configuration\n",
    "\n",
    "We use the following configuration in our benchmarks:\n",
    "### Test cases \n",
    "- full computation over the entire dataset (upper baseline)\n",
    "- progressive computation without optimization (lower baseline)\n",
    "- full computation of processed data\n",
    "- progressive computation using optimization strategies\n",
    "\n",
    "### Dataset\n",
    "- NYC taxis dataset (10 Million items), stored in a compressed CSV file, loaded with DuckDB \n",
    "\n",
    "### Variables\n",
    "- dependent variables: runtime, prediction error\n",
    "- independent variables: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the path to be able to import the local modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some contant values for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import ID, initialize_db, drop_tables, get_next_chunk_from_db, save_dois, get_from_doi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "n_dims = 17\n",
    "total_items = 99999\n",
    "chunk_size = 1000\n",
    "chunks = round(total_items / chunk_size)\n",
    "\n",
    "def reset():\n",
    "  drop_tables()\n",
    "  initialize_db(\"../data/nyc_taxis_sampled100k_shuffled.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DOI function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doi_component.outlierness_component import OutliernessComponent\n",
    "\n",
    "outlierness = OutliernessComponent([\"ratio\", \"duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: Chunk-based computation without any optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()\n",
    "\n",
    "# lower baseline: chunk-based computation over the processed data so far.\n",
    "lower_bound_result = pd.DataFrame([])\n",
    "start = time.time()\n",
    "for i in range(chunks):\n",
    "  chunk = get_next_chunk_from_db(chunk_size, as_df=True)\n",
    "  doi = outlierness.compute_doi(chunk)\n",
    "  lower_bound_result = lower_bound_result.append(pd.DataFrame(doi))\n",
    "\n",
    "time_lower = time.time() - start\n",
    "\n",
    "print(f\"# lower bound: {time_lower}\")\n",
    "lower_bound_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()\n",
    "\n",
    "# upper baseline: full computation over the processed data so far.\n",
    "start = time.time()\n",
    "data = get_next_chunk_from_db(chunk_size * chunks, as_df=True)\n",
    "upper_bound_result = outlierness.compute_doi(data)\n",
    "time_upper = time.time() - start\n",
    "\n",
    "print(f\"# upper bound: {time_upper}\")\n",
    "upper_bound_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update selection strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from outdated_item_selection_strategy.no_update import *\n",
    "from outdated_item_selection_strategy.oldest_chunks_update import *\n",
    "from outdated_item_selection_strategy.last_n_chunks_update import *\n",
    "from outdated_item_selection_strategy.regular_interval_update import *\n",
    "from outdated_item_selection_strategy.outdated_bin_update import *\n",
    "\n",
    "update_strategies = [\n",
    "  (\"no chunk\", NoUpdate(n_dims=n_dims)),\n",
    "  (\"oldest n chunks\", OldestChunksUpdate(n_dims=n_dims, n_chunks=3, max_age=10)),\n",
    "  (\"last n chunks\", LastNChunksUpdate(n_dims=n_dims, n_chunks=3)),\n",
    "  (\"regular intervals\", RegularIntervalUpdate(n_dims=n_dims,interval=2, max_age=10)),\n",
    "  (\"outdated bins\", OutdatedBinUpdate(n_dims=n_dims))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from database import ID, initialize_db, drop_tables, get_next_chunk_from_db, save_dois, get_from_doi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "n_dims = 17\n",
    "total_items = 99999\n",
    "chunk_size = 1000\n",
    "chunks = round(total_items / chunk_size)\n",
    "\n",
    "def reset():\n",
    "  drop_tables()\n",
    "  initialize_db(\"../data/nyc_taxis_sampled100k_shuffled.csv.gz\")\n",
    "\n",
    "from doi_component.outlierness_component import OutliernessComponent\n",
    "\n",
    "outlierness = OutliernessComponent([\"ratio\", \"duration\"])\n",
    "\n",
    "from outdated_item_selection_strategy.no_update import *\n",
    "from outdated_item_selection_strategy.oldest_chunks_update import *\n",
    "from outdated_item_selection_strategy.last_n_chunks_update import *\n",
    "from outdated_item_selection_strategy.regular_interval_update import *\n",
    "from outdated_item_selection_strategy.outdated_bin_update import *\n",
    "\n",
    "update_strategies = [\n",
    "  (\"no chunk\", NoUpdate(n_dims=n_dims)),\n",
    "  (\"oldest n chunks\", OldestChunksUpdate(n_dims=n_dims, n_chunks=3, max_age=10)),\n",
    "  (\"last n chunks\", LastNChunksUpdate(n_dims=n_dims, n_chunks=3)),\n",
    "  (\"regular intervals\", RegularIntervalUpdate(n_dims=n_dims,interval=2, max_age=10)),\n",
    "  (\"outdated bins\", OutdatedBinUpdate(n_dims=n_dims))\n",
    "]\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "from database import ID,process_chunk, update_dois, save_dois\n",
    "reset()\n",
    "# lower baseline: chunk-based computation over the processed data so far.\n",
    "start = time.time()\n",
    "\n",
    "strategy = update_strategies[1]\n",
    "print(\"strategy:\", strategy[0])\n",
    "\n",
    "# total time per chunk\n",
    "# time to retrieve the next chunk\n",
    "# time to compute the update\n",
    "# time to compute the context\n",
    "# time to compute the doi function over data+context\n",
    "# time to compute the doi function over data+update\n",
    "print(\"chunk, chunk_time, outdated_time, new_doi_time, old_doi_time, store_new_time, update_dois_time, step_time\")\n",
    "for i in range(chunks):\n",
    "  step_time = time.time()\n",
    "  # print(f\"({i}/{chunks})\")\n",
    "  \n",
    "  before = time.time()\n",
    "  chunk = get_next_chunk_from_db(chunk_size, as_df=True)\n",
    "  outdated = strategy[1].get_outdated_items(i)\n",
    "  chunk_time = time.time() - before\n",
    "\n",
    "  before = time.time()\n",
    "  outdated = process_chunk(pd.DataFrame(outdated))\n",
    "  outdated_time = time.time() - before\n",
    "  \n",
    "  # append the context items to chunk\n",
    "  # chunk = chunk.append(context)\n",
    "  new_ids = chunk[ID].to_list()\n",
    "  old_ids = outdated[ID].to_list()\n",
    "\n",
    "  before = time.time()\n",
    "  new_doi = outlierness.compute_doi(chunk)\n",
    "  new_doi_time = time.time() - before\n",
    "\n",
    "  before = time.time()\n",
    "  save_dois(new_ids, new_doi, np.zeros_like(new_doi))\n",
    "  store_new_time = time.time() - before\n",
    "\n",
    "  before = time.time()\n",
    "  old_doi = outlierness.compute_doi(chunk.append(outdated))[len(chunk):]\n",
    "  old_doi_time = time.time() - before\n",
    "  \n",
    "  # print(len(old_ids), len(old_doi))\n",
    "\n",
    "  before = time.time()\n",
    "  update_dois(old_ids, old_doi)\n",
    "  update_dois_time = time.time() - before\n",
    "  # print(f\"updating: {time.time() - before}s\")\n",
    "\n",
    "  step_time = time.time() - step_time\n",
    "\n",
    "  print(i, chunk_time, outdated_time, new_doi_time, old_doi_time, store_new_time, update_dois_time, step_time)\n",
    "\n",
    "time_strat = time.time() - start\n",
    "start_result = get_from_doi([\"TRUE\"], as_df=True)\n",
    "\n",
    "print(f\"# using {strategy[0]} strategy: {time_strat}\")\n",
    "start_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context selection strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_item_selection_strategy.chunk_based_context import *\n",
    "from context_item_selection_strategy.sampling_based_context import *\n",
    "from context_item_selection_strategy.clustering_based_context import *\n",
    "from context_item_selection_strategy.no_context import * \n",
    "\n",
    "context_strategies = [\n",
    "  (\"no context\", NoContext(n_dims=n_dims)),\n",
    "  (\"chunk based\", RandomChunkBasedContext(n_dims=n_dims, n_chunks=3)),\n",
    "  (\"sampling based\", RandomSamplingBasedContext(n_dims=n_dims, n_samples=chunk_size)),\n",
    "  (\"clustering based\", ClusteringBasedContext(n_dims=n_dims, n_clusters=chunk_size))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chunk = chunks\n",
    "context_size = chunk_size\n",
    "\n",
    "for i, strategy in enumerate(context_strategies):\n",
    "  start = time.time()\n",
    "  print(\"#\", strategy[0])\n",
    "  context_items = strategy[1].get_context_items(current_chunk)\n",
    "  print(f\"found {len(context_items)} context items:\")\n",
    "  print(context_items)\n",
    "  print(time.time() - start)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import ID, process_chunk\n",
    "reset()\n",
    "# lower baseline: chunk-based computation over the processed data so far.\n",
    "start = time.time()\n",
    "\n",
    "strategy = strategies[1]\n",
    "\n",
    "for i in range(chunks):\n",
    "  chunk = get_next_chunk_from_db(chunk_size, as_df=True)\n",
    "  new_entries = len(chunk)\n",
    "  context = strategy[1].get_context_items(i)\n",
    "  context = process_chunk(context)\n",
    "  chunk = chunk.append(context)\n",
    "  doi = outlierness.compute_doi(chunk)\n",
    "  new_ids = chunk[ID][:new_entries].to_list()\n",
    "  new_dois = doi[:new_entries]\n",
    "  save_dois(new_ids, new_dois, np.zeros_like(new_dois))\n",
    "\n",
    "time_strat = time.time() - start\n",
    "start_result = get_from_doi([\"TRUE\"], as_df=True)\n",
    "\n",
    "print(f\"# using {strategy[0]} strategy: {time_strat}\")\n",
    "start_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy: no chunk\n",
      "strategy: oldest n chunks\n",
      "strategy: last n chunks\n",
      "strategy: regular intervals\n",
      "strategy: outdated bins\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sys import path\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from database import ID, DOI, initialize_db, drop_tables, get_next_chunk_from_db, save_dois, get_from_doi\n",
    "from benchmark_test_case import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "n_dims = 17\n",
    "# total_items = 99999\n",
    "total_items = 999999\n",
    "chunk_size = 1000\n",
    "chunks = round(total_items / chunk_size)\n",
    "\n",
    "def reset():\n",
    "  drop_tables()\n",
    "  initialize_db(\"../data/nyc_taxis_sampled100k_shuffled.csv.gz\")\n",
    "\n",
    "from doi_component.outlierness_component import OutliernessComponent\n",
    "\n",
    "outlierness = OutliernessComponent([\"ratio\", \"duration\"])\n",
    "\n",
    "from outdated_item_selection_strategy.no_update import *\n",
    "from outdated_item_selection_strategy.oldest_chunks_update import *\n",
    "from outdated_item_selection_strategy.last_n_chunks_update import *\n",
    "from outdated_item_selection_strategy.regular_interval_update import *\n",
    "from outdated_item_selection_strategy.outdated_bin_update import *\n",
    "\n",
    "update_strategies = [\n",
    "  (\"no chunk\", NoUpdate(n_dims=n_dims)),\n",
    "  (\"oldest n chunks\", OldestChunksUpdate(n_dims=n_dims, n_chunks=3, max_age=10)),\n",
    "  (\"last n chunks\", LastNChunksUpdate(n_dims=n_dims, n_chunks=3)),\n",
    "  (\"regular intervals\", RegularIntervalUpdate(n_dims=n_dims,interval=2, max_age=10)),\n",
    "  (\"outdated bins\", OutdatedBinUpdate(n_dims=n_dims))\n",
    "]\n",
    "\n",
    "from context_item_selection_strategy.no_context import *\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "from database import ID,process_chunk, update_dois, save_dois\n",
    "\n",
    "charts = []\n",
    "for strategy in update_strategies:\n",
    "  reset()\n",
    "  print(\"strategy:\", strategy[0])\n",
    "  test_case = BenchmarkTestCase(strategy[0], outlierness, NoContext(n_dims), strategy[1], chunk_size, chunks)\n",
    "  test_case.run()\n",
    "\n",
    "charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 17\n",
    "total_items = 99999\n",
    "chunk_size = 1000\n",
    "chunks = round(total_items / chunk_size)\n",
    "outlierness = OutliernessComponent([\"ratio\", \"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_case = MonolithicComputationTestCase(\"full computation\", outlierness, chunk_size*chunks, 1)\n",
    "results_full = full_test_case.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_item_selection_strategy.chunk_based_context import *\n",
    "chunk_context = MostRecentChunkBasedContext(n_dims=n_dims, n_chunks=3)\n",
    "context_test_case = ContextStrategyTestCase(\"context\", chunk_context, outlierness, chunk_size, chunks)\n",
    "results_context = context_test_case.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_test_case = MonolithicComputationTestCase(\"only chunks\", outlierness, chunk_size, chunks)\n",
    "results_chunked = chunked_test_case.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import jaccard_score, r2_score\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "def evaluate_test_case(test_case: np.ndarray, ground_truth: np.ndarray):\n",
    "  # score = jaccard_score(test_case, ground_truth, average=\"weighted\")\n",
    "  score = r2_score(ground_truth, test_case)\n",
    "  return score\n",
    "\n",
    "ground_truth = results_full[\"doi\"]\n",
    "context_test_case = results_context[\"doi\"]\n",
    "baseline_test_case = results_chunked[\"doi\"]\n",
    "\n",
    "\n",
    "evaluate_test_case(baseline_test_case, ground_truth), evaluate_test_case(context_test_case, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
