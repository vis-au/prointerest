{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks of Strategies for Selecting Outdated Items\n",
    "This notebook contains the benchmarks related to the selection strategies for context data, which we report in our paper.\n",
    "Context data are selected from the processed data and included in the next progressive computation step, such that its result approximates that of a _non-progressive_ computation over the processed data.\n",
    "\n",
    "## Benchmark Configuration\n",
    "\n",
    "We use the following configuration in our benchmarks:\n",
    "### Test cases \n",
    "- full computation over the entire dataset (upper baseline)\n",
    "- progressive computation without optimization (lower baseline)\n",
    "- full computation of processed data\n",
    "- progressive computation using optimization strategies\n",
    "\n",
    "### Dataset\n",
    "- NYC taxis dataset (10 Million items), stored in a compressed CSV file, loaded with DuckDB \n",
    "\n",
    "### Variables\n",
    "- dependent variables: runtime, prediction error\n",
    "- independent variables: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_item_selection_strategy.chunk_based_context import *\n",
    "from context_item_selection_strategy.sampling_based_context import *\n",
    "from context_item_selection_strategy.clustering_based_context import *\n",
    "\n",
    "n_dims = 17\n",
    "chunk_size = 100\n",
    "\n",
    "strategies = [\n",
    "  (\"chunk based\", ChunkBasedContext(n_dims=n_dims, n_chunks=3)),\n",
    "  (\"sampling based\", SamplingBasedContext(n_dims=n_dims, n_samples=chunk_size)),\n",
    "  (\"clustering based\", ClusteringBasedContext(n_dims=n_dims, n_clusters=chunk_size))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization took 0.10671401023864746\n",
      "loading the data in chunks took 1.4261481761932373\n"
     ]
    }
   ],
   "source": [
    "from database import get_next_chunk_from_db, initialize_db, drop_tables\n",
    "import time\n",
    "\n",
    "chunks = 15\n",
    "chunk_size = 100\n",
    "\n",
    "start = time.time()\n",
    "drop_tables()\n",
    "initialize_db(\"../data/nyc_taxis_sampled100k_shuffled.csv.gz\")\n",
    "print(\"initialization took\", time.time() - start)\n",
    "\n",
    "s = time.time()\n",
    "for i in range(chunks):\n",
    "  chunk = get_next_chunk_from_db(chunk_size)\n",
    "print(\"loading the data in chunks took\", time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# chunk based\n",
      "found 200 context items:\n",
      "[[52792702 2 Timestamp('2018-10-06 11:40:12') ... 0.0 0.3 32.76]\n",
      " [66930640 2 Timestamp('2018-11-26 23:09:53') ... 5.76 0.3 32.14]\n",
      " [37619021 1 Timestamp('2018-08-08 23:01:25') ... 0.0 0.3 8.8]\n",
      " ...\n",
      " [31466697 2 Timestamp('2018-07-16 18:06:22') ... 0.0 0.3 7.82]\n",
      " [73877432 2 Timestamp('2018-12-20 20:35:12') ... 0.0 0.3 15.96]\n",
      " [61365300 1 Timestamp('2018-11-05 18:49:57') ... 0.0 0.3 23.92]]\n",
      "0.2743861675262451\n",
      "\n",
      "\n",
      "# sampling based\n",
      "found 100 context items:\n",
      "[[75736837 1 Timestamp('2018-12-29 18:49:57') ... 0.0 0.3 7.3]\n",
      " [42552891 2 Timestamp('2018-08-29 07:20:23') ... 0.0 0.3 61.56]\n",
      " [3063068 1 Timestamp('2018-05-20 06:35:00') ... 0.0 0.3 23.15]\n",
      " ...\n",
      " [11419837 1 Timestamp('2018-06-18 21:15:46') ... 0.0 0.3\n",
      "  7.5600000000000005]\n",
      " [54511485 2 Timestamp('2018-10-12 17:47:23') ... 0.0 0.3 40.56]\n",
      " [53541945 2 Timestamp('2018-10-09 12:06:06') ... 0.0 0.3 29.75]]\n",
      "0.2671966552734375\n",
      "\n",
      "\n",
      "# clustering based\n",
      "found 100 context items:\n",
      "[[61726465 2 Timestamp('2018-11-07 01:06:10') ... 0.0 0.3 8.16]\n",
      " [25466373 1 Timestamp('2018-02-07 08:49:08') ... 0.0 0.3 21.3]\n",
      " [7318198 2 Timestamp('2018-06-04 18:29:39') ... 0.0 0.3 9.96]\n",
      " ...\n",
      " [10428271 2 Timestamp('2018-06-15 08:41:34') ... 0.0 0.3 8.5]\n",
      " [50816514 1 Timestamp('2018-09-29 11:53:04') ... 0.0 0.3 6.8]\n",
      " [47875160 1 Timestamp('2018-09-18 13:14:12') ... 0.0 0.3 9.8]]\n",
      "23.80283784866333\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = chunks\n",
    "for i, strategy in enumerate(strategies):\n",
    "  start = time.time()\n",
    "  print(\"#\", strategy[0])\n",
    "  context_items = strategy[1].get_context_items(current_chunk)\n",
    "  print(f\"found {len(context_items)} context items:\")\n",
    "  print(context_items)\n",
    "  print(time.time() - start)\n",
    "  print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
