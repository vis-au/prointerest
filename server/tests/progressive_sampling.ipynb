{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive Sampling\n",
    "A common challenge for computing doi values progressively is that the dataset processed so far will eventually become too large, causing the doi computation too take too long between chunks.\n",
    "The quick solution is to maintain a representative sample of the processed data that is of a constant maximum size, on which the computation is run instead. \n",
    "The problem here is that doi values are often unevenly distributed, which means that a random sampling might not catch those \"rare\" items that the user is actually interested in.\n",
    "\n",
    "The general idea we follow here is to divide the data into bins of equal density, based on their doi value. \n",
    "Then, we progressively maintain a representative set of points for each bin, to be able to capture the distributions of the entire dataset whenever we recompute the doi function or train the classifier.\n",
    "\n",
    "This task is split into the following subtasks: \n",
    "1. compute the doi function over all items across all bins\n",
    "2. rebin the data based on their doi values\n",
    "3. update items in bins that have changed \"too much\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Compute the doi function\n",
    "For simplicity, we will use outlier detection here, which for every item computes, whether or not it is an outlier in the dataset, given an approximate \"contamination\" of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sys import path\n",
    "cwd = os.getcwd()\n",
    "path.append(f\"{cwd}/..\")\n",
    "\n",
    "from doi_component.outlierness_component import OutliernessComponent\n",
    "\n",
    "doi_comp = OutliernessComponent()\n",
    "\n",
    "def doi(X: np.ndarray):\n",
    "  df = pd.DataFrame(X)\n",
    "  return doi_comp.compute_doi(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Binning\n",
    "Given a distribution of floating point values, progressively maintain bins of equal density for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 10000\n",
    "training_size = 100\n",
    "features = 2\n",
    "chunk_size = 1000\n",
    "n_bins = 5\n",
    "\n",
    "blobs_params = {\"n_samples\": N, \"n_features\": features}\n",
    "hist_params = {\"alpha\": 0.73, \"ec\": \"#000\"}\n",
    "\n",
    "binning = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "X = make_blobs(centers=6, cluster_std=1, **blobs_params)[0]\n",
    "y = doi(X)\n",
    "\n",
    "distributions = [np.random.normal(random(), random() * 0.25, N) for _ in range(N//chunk_size)]\n",
    "previous_hist = []\n",
    "\n",
    "for i, distribution in enumerate(distributions):\n",
    "  y_ = y + distribution\n",
    "\n",
    "  binning.fit(y_.reshape(-1, 1))\n",
    "  edges = binning.bin_edges_[0]\n",
    "  bins = binning.transform(y_.reshape(-1, 1))\n",
    "  bins = binning.inverse_transform(bins)\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "  hist = np.histogram(y_, bins=edges)[0]\n",
    "  if i > 0:\n",
    "    delta_hist = hist - previous_hist\n",
    "    ax2.bar(np.arange(0, n_bins), delta_hist, fc=\"orange\", **hist_params)\n",
    "    ax2.axhline(0, lw=1, c=\"black\")\n",
    "    ax2.axhline(N * 0.1, lw=1, c=\"black\", dashes=[2])\n",
    "    ax2.axhline(-N * 0.1, lw=1, c=\"black\", dashes=[2])\n",
    "    ax2.set_ylim([-N, N])\n",
    "  previous_hist = hist\n",
    "\n",
    "  ax1.set_title(\"chunk\")\n",
    "  ax1.set_ylim([0, N])\n",
    "  ax1.hist(bins, bins=edges, fc=\"steelblue\", **hist_params)\n",
    "  ax1.hist(y_, fc=\"white\", **hist_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Sampling\n",
    "A good candidate seems to be so-called reservoir sampling, which progressively updates a representative sample of the data seen so far, thus at every point maintainnig a sample where each point may have been picked with probability := 1/(number of points processed so far).\n",
    "\n",
    "See [https://dl.acm.org/doi/pdf/10.1145/3147.3165](https://dl.acm.org/doi/pdf/10.1145/3147.3165)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive chunk-based Reservoir sampling\n",
    "Pseudo code:\n",
    "```\n",
    "# we progress over a dataset X of size N and want to maintain a progressive random sample S of \n",
    "# size n.\n",
    "# Every item x_t from the dataset is numbered based on the timepoint t at which it is \n",
    "# processed: x_t with t \\in [0 ... N]. We go through the data in linear order.\n",
    "# for x_t in X:\n",
    "  # if t <= n, then x_t is put into S.\n",
    "  # otherwise, x_t becomes a candidate that may replace an item in S:\n",
    "  #    generate a random integer r from the interval [0, t]. If r < n, then replace the item S[r] \n",
    "  #    with with x_t\n",
    "# the probability of an item becoming part of the sample S is n/t and is decreasing with t.\n",
    "# the probability of being replaced is higher, the longer an item is part of Sy.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.random import sample_without_replacement\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "reservoir = np.array([])\n",
    "k = 5\n",
    "\n",
    "X = np.arange(0, 1000)\n",
    "chunk_size = 10\n",
    "\n",
    "for i in range(0, len(X) // chunk_size):\n",
    "  chunk = X[i * chunk_size:(i+1) * chunk_size]\n",
    "\n",
    "  if i == 0:\n",
    "    sample = sample_without_replacement(n_population=chunk_size, n_samples=k, random_state=i)\n",
    "    reservoir = chunk[sample]\n",
    "  else:\n",
    "    candidates = np.array([randint(0, i*chunk_size) for _ in range(chunk_size)])\n",
    "    picks = candidates[candidates < k]\n",
    "    reservoir[picks] = chunk[picks]\n",
    "\n",
    "reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimized chunk-based Reservoir Sampling\n",
    "\n",
    "Quote from the reservoir sampling paper:\n",
    "> The random variable S(n, t) is defined to be the number of records in the file that are skipped over before the next record is chosen for the reservoir, where n is the size of the sample and where t is the number of records processed so far. For notational simplicity, we shall often write 9 for S(n, t),in which case the parameters n and t will be implicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.random import sample_without_replacement\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "reservoir = np.array([])\n",
    "k = 5\n",
    "\n",
    "X = np.arange(0, 1000)\n",
    "chunk_size = 10\n",
    "\n",
    "skip = k\n",
    "\n",
    "for i in range(0, len(X) // chunk_size):\n",
    "  chunk = X[i * chunk_size:(i+1) * chunk_size]\n",
    "\n",
    "  if i == 0:\n",
    "    sample = sample_without_replacement(n_population=chunk_size, n_samples=k, random_state=i)\n",
    "    reservoir = chunk[sample]\n",
    "    skip = randint(k, (i+1)*chunk_size)\n",
    "  elif skip < i*chunk_size:\n",
    "    candidate = chunk[skip % chunk_size]\n",
    "    reservoir[randint(0, k-1)] = candidate\n",
    "    skip = randint(k, i*chunk_size*2)\n",
    "\n",
    "reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Progressive Bin-based Sampling with Balanced Classes\n",
    "Putting things together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar():\n",
    "  # plot the results\n",
    "  fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(7, 7), gridspec_kw={\"hspace\": 1.25})\n",
    "  ax1.set_title(\"ground truth y values\")\n",
    "  ax1.hist(y, **hist_params)\n",
    "\n",
    "  ax3.set_title(\"doi computed from sampled X\")\n",
    "  reservoir = np.concatenate(tuple([bin_sampler.X_reservoir[x] for x in bin_sampler.X_reservoir]))\n",
    "  sampled_X_y = doi(reservoir)\n",
    "  ax3.hist(sampled_X_y, **hist_params)\n",
    "\n",
    "def plot_scatter():\n",
    "  reservoir_X = np.concatenate(tuple([bin_sampler.X_reservoir[x] for x in bin_sampler.X_reservoir]))\n",
    "  reservoir_y = doi(reservoir_X)\n",
    "  chunk_y = doi(chunk)\n",
    "  fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 10))\n",
    "  ax1.set_title(\"reservoir\")\n",
    "  ax1.scatter(x=reservoir_X[:,0], y=reservoir_X[:,1], c=reservoir_y, alpha=0.1)\n",
    "  ax2.set_title(\"chunk\")\n",
    "  ax2.scatter(x=chunk[:,0], y=chunk[:,1], c=chunk_y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin_reservoirs(processed):\n",
    "  '''\n",
    "  Recompute the doi function over all items in the reservoir, then redistribute the data into bins \n",
    "  again.\n",
    "  '''\n",
    "  X_ = bin_sampler.get_current_sample()\n",
    "  y_ = doi(X_)\n",
    "\n",
    "  bin_sampler.reset_reservoirs()\n",
    "\n",
    "  return bin_sampler.add_chunk(X_, y_, processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import numpy as np\n",
    "from random import random\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "import matplotlib.pyplot as plt\n",
    "from storage_strategy.progressive_bin_sampler import ProgressiveBinSampler\n",
    "\n",
    "N = 10000\n",
    "features = 2\n",
    "chunk_size = 1000\n",
    "\n",
    "chunks = N // chunk_size\n",
    "\n",
    "blobs_params = {\"n_samples\": N, \"n_features\": features}\n",
    "hist_params = {\"alpha\": 0.73, \"ec\": \"#000\"}\n",
    "binning = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"kmeans\")\n",
    "\n",
    "X = make_blobs(centers=6, cluster_std=1, **blobs_params)[0]\n",
    "y = doi(X)\n",
    "\n",
    "# last iteration's per-bin mean values\n",
    "previous_bin_means = np.array([])\n",
    "\n",
    "# sum of squared differences betwen partial doi and full doi computation\n",
    "mse = []\n",
    "\n",
    "bin_sampler = ProgressiveBinSampler()\n",
    "\n",
    "for i in range(chunks):\n",
    "  print(\"\\n#chunk:\", i)\n",
    "\n",
    "  chunk = X[i*chunk_size:(i+1)*chunk_size]\n",
    "\n",
    "  # compute the doi function over known items and the items in the new chunk\n",
    "  progressive_sample = bin_sampler.get_current_sample()\n",
    "  X_ = np.append(progressive_sample, chunk, axis=0)\n",
    "  y_ = doi(X_)\n",
    "\n",
    "  bin_sampler.add_chunk(X_[-chunk_size:], y_[-chunk_size:], i*chunk_size)\n",
    "\n",
    "\n",
    "  # TODO: check if rebinning is necessary\n",
    "  # compute the prediction error for each chunk\n",
    "  error = np.mean((y_[-chunk_size:]-y[i*chunk_size:(i+1)*chunk_size])**2)\n",
    "  mse += [error]\n",
    "\n",
    "  # rebin every 4 iterations\n",
    "  if i > 0 and i % 4 == 0:\n",
    "    rebin_reservoirs(i*chunk_size)\n",
    "  \n",
    "  plot_scatter()\n",
    "\n",
    "print(\"error progression:\", mse)\n",
    "plot_bar()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
