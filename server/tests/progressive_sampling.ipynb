{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Progressive Sampling\r\n",
    "The general idea is to divide the data into bins of equal density. \r\n",
    "Then, we progressively maintain a representative set of points for each bin, to be able to capture the distributions of the entire dataset whenever we recompute the doi function or train the classifier.\r\n",
    "\r\n",
    "This task is split into the following subtasks: \r\n",
    "1. compute the doi function over all items across all bins\r\n",
    "2. rebin the data based on their doi values\r\n",
    "3. update items in bins that have changed \"too much\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Compute the doi function\r\n",
    "For simplicity, we will use outlier detection here, which for every item computes, whether or not it is an outlier in the dataset, given an approximate \"contamination\" of the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.ensemble import IsolationForest\r\n",
    "doi = IsolationForest(contamination=0.1, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Binning\r\n",
    "Given a distribution of floating point values, progressively maintain bins of equal density for them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import numpy as np\r\n",
    "from random import random\r\n",
    "from sklearn.datasets import make_blobs\r\n",
    "from sklearn.preprocessing import KBinsDiscretizer\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "N = 10000\r\n",
    "training_size = 100\r\n",
    "features = 2\r\n",
    "chunk_size = 1000\r\n",
    "\r\n",
    "blobs_params = dict(n_samples=N, n_features=features)\r\n",
    "X = make_blobs(centers=6, cluster_std=1, **blobs_params)[0]\r\n",
    "y = doi.fit_predict(X)\r\n",
    "\r\n",
    "n_bins = 5\r\n",
    "binning = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"uniform\")\r\n",
    "edges = binning.fit(y.reshape(-1, 1)).bin_edges_[0]\r\n",
    "\r\n",
    "previous_bins = []\r\n",
    "\r\n",
    "distributions = [np.random.normal(random(), random() * 0.25, N) for _ in range(N//chunk_size)]\r\n",
    "for i, distribution in enumerate(distributions):\r\n",
    "  y_ = y + distribution\r\n",
    "  binning.fit(y_.reshape(-1, 1))\r\n",
    "  \r\n",
    "  fig, (ax1, ax2, ax3) = plt.subplots(3, 1)\r\n",
    "  bins = binning.transform(y_.reshape(-1, 1))\r\n",
    "  if i > 0:\r\n",
    "    delta_bins = bins - previous_bins\r\n",
    "    ax3.hist(delta_bins, fc=\"green\", bins=edges, **hist_params)\r\n",
    "\r\n",
    "  previous_bins = bins\r\n",
    "\r\n",
    "  bins = bins.flatten().astype(int)\r\n",
    "  bins = np.array([edges[bins[i]] for i in range(len(bins))])\r\n",
    "  \r\n",
    "  hist_params = {\"alpha\": .7, \"ec\": \"#000\"}\r\n",
    "  ax1.set_title(\"original data\")\r\n",
    "  ax1.hist(y_, **hist_params)\r\n",
    "  ax2.hist(bins, fc=\"orange\", bins=edges, **hist_params)\r\n",
    "  \r\n",
    "\r\n",
    "  previous_bins = bins"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Sampling\r\n",
    "A good candidate seems to be so-called reservoir sampling, which progressively updates a representative sample of the data seen so far, thus at every point maintainnig a sample where each point may have been picked with probability := 1/(number of points processed so far).\r\n",
    "\r\n",
    "See [https://dl.acm.org/doi/pdf/10.1145/3147.3165](https://dl.acm.org/doi/pdf/10.1145/3147.3165)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive chunk-based Reservoir sampling\r\n",
    "Pseudo code:\r\n",
    "```\r\n",
    "# we progress over a dataset X of size N and want to maintain a progressive random sample S of \r\n",
    "# size n.\r\n",
    "# Every item x_t from the dataset is numbered based on the timepoint t at which it is \r\n",
    "# processed: x_t with t \\in [0 ... N]. We go through the data in linear order.\r\n",
    "# for x_t in X:\r\n",
    "  # if t <= n, then x_t is put into S.\r\n",
    "  # otherwise, x_t becomes a candidate that may replace an item in S:\r\n",
    "  #    generate a random integer r from the interval [0, t]. If r < n, then replace the item S[r] \r\n",
    "  #    with with x_t\r\n",
    "# the probability of an item becoming part of the sample S is n/t and is decreasing with t.\r\n",
    "# the probability of being replaced is higher, the longer an item is part of Sy.\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "from sklearn.utils.random import sample_without_replacement\r\n",
    "from random import randint\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "reservoir = np.array([])\r\n",
    "k = 5\r\n",
    "\r\n",
    "X = np.arange(0, 1000)\r\n",
    "chunk_size = 10\r\n",
    "\r\n",
    "for i in range(0, len(X) // chunk_size):\r\n",
    "  chunk = X[i * chunk_size:(i+1) * chunk_size]\r\n",
    "\r\n",
    "  if i == 0:\r\n",
    "    sample = sample_without_replacement(n_population=chunk_size, n_samples=k, random_state=i)\r\n",
    "    reservoir = chunk[sample]\r\n",
    "  else:\r\n",
    "    candidates = np.array([randint(0, i*chunk_size) for _ in range(chunk_size)])\r\n",
    "    picks = candidates[candidates < k]\r\n",
    "    reservoir[picks] = chunk[picks]\r\n",
    "\r\n",
    "reservoir"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([510, 301, 192, 573, 944])"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimized chunk-based Reservoir Sampling\r\n",
    "\r\n",
    "Quote from the reservoir sampling paper:\r\n",
    "> The random variable S(n, t) is defined to be the number of records in the file that are skipped over before the next record is chosen for the reservoir, where n is the size of the sample and where t is the number of records processed so far. For notational simplicity, we shall often write 9 for S(n, t),in which case the parameters n and t will be implicit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "from sklearn.utils.random import sample_without_replacement\r\n",
    "from random import randint\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "reservoir = np.array([])\r\n",
    "k = 5\r\n",
    "\r\n",
    "X = np.arange(0, 1000)\r\n",
    "chunk_size = 10\r\n",
    "\r\n",
    "skip = k\r\n",
    "\r\n",
    "for i in range(0, len(X) // chunk_size):\r\n",
    "  chunk = X[i * chunk_size:(i+1) * chunk_size]\r\n",
    "\r\n",
    "  if i == 0:\r\n",
    "    sample = sample_without_replacement(n_population=chunk_size, n_samples=k, random_state=i)\r\n",
    "    reservoir = chunk[sample]\r\n",
    "    skip = randint(k, (i+1)*chunk_size)\r\n",
    "  elif skip < i*chunk_size:\r\n",
    "    candidate = chunk[skip % chunk_size]\r\n",
    "    reservoir[randint(0, k-1)] = candidate\r\n",
    "    skip = randint(k, i*chunk_size*2)\r\n",
    "\r\n",
    "reservoir"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([262, 104, 972, 996, 984])"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}