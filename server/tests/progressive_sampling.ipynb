{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Progressive Sampling\r\n",
    "The general idea is to divide the data into bins of equal density. \r\n",
    "Then, we progressively maintain a representative set of points for each bin, to be able to capture the distributions of the entire dataset whenever we recompute the doi function or train the classifier.\r\n",
    "\r\n",
    "This task is split into the following subtasks: \r\n",
    "1. compute the doi function over all items across all bins\r\n",
    "2. rebin the data based on their doi values\r\n",
    "3. update items in bins that have changed \"too much\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Compute the doi function\r\n",
    "For simplicity, we will use outlier detection here, which for every item computes, whether or not it is an outlier in the dataset, given an approximate \"contamination\" of the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sys import path\r\n",
    "path.append(\"../\")\r\n",
    "\r\n",
    "from outlierness_component import OutliernessComponent\r\n",
    "\r\n",
    "doi = OutliernessComponent()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Binning\r\n",
    "Given a distribution of floating point values, progressively maintain bins of equal density for them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "from random import random\r\n",
    "from sklearn.datasets import make_blobs\r\n",
    "from sklearn.preprocessing import KBinsDiscretizer\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "N = 10000\r\n",
    "training_size = 100\r\n",
    "features = 2\r\n",
    "chunk_size = 1000\r\n",
    "n_bins = 5\r\n",
    "\r\n",
    "blobs_params = {\"n_samples\": N, \"n_features\": features}\r\n",
    "hist_params = {\"alpha\": 0.73, \"ec\": \"#000\"}\r\n",
    "\r\n",
    "binning = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"uniform\")\r\n",
    "X = make_blobs(centers=6, cluster_std=1, **blobs_params)[0]\r\n",
    "y = doi.fit_predict(X)\r\n",
    "\r\n",
    "distributions = [np.random.normal(random(), random() * 0.25, N) for _ in range(N//chunk_size)]\r\n",
    "previous_hist = []\r\n",
    "\r\n",
    "for i, distribution in enumerate(distributions):\r\n",
    "  y_ = y + distribution\r\n",
    "\r\n",
    "  binning.fit(y_.reshape(-1, 1))\r\n",
    "  edges = binning.bin_edges_[0]\r\n",
    "  bins = binning.transform(y_.reshape(-1, 1))\r\n",
    "  bins = binning.inverse_transform(bins)\r\n",
    "\r\n",
    "  fig, (ax1, ax2) = plt.subplots(2, 1)\r\n",
    "\r\n",
    "  hist = np.histogram(y_, bins=edges)[0]\r\n",
    "  if i > 0:\r\n",
    "    delta_hist = hist - previous_hist\r\n",
    "    ax2.bar(np.arange(0, n_bins), delta_hist, fc=\"orange\", **hist_params)\r\n",
    "    ax2.axhline(0, lw=1, c=\"black\")\r\n",
    "    ax2.axhline(N * 0.1, lw=1, c=\"black\", dashes=[2])\r\n",
    "    ax2.axhline(-N * 0.1, lw=1, c=\"black\", dashes=[2])\r\n",
    "    ax2.set_ylim([-N, N])\r\n",
    "  previous_hist = hist\r\n",
    "\r\n",
    "  ax1.set_title(\"chunk\")\r\n",
    "  ax1.set_ylim([0, N])\r\n",
    "  ax1.hist(bins, bins=edges, fc=\"steelblue\", **hist_params)\r\n",
    "  ax1.hist(y_, fc=\"white\", **hist_params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Sampling\r\n",
    "A good candidate seems to be so-called reservoir sampling, which progressively updates a representative sample of the data seen so far, thus at every point maintainnig a sample where each point may have been picked with probability := 1/(number of points processed so far).\r\n",
    "\r\n",
    "See [https://dl.acm.org/doi/pdf/10.1145/3147.3165](https://dl.acm.org/doi/pdf/10.1145/3147.3165)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Naive chunk-based Reservoir sampling\r\n",
    "Pseudo code:\r\n",
    "```\r\n",
    "# we progress over a dataset X of size N and want to maintain a progressive random sample S of \r\n",
    "# size n.\r\n",
    "# Every item x_t from the dataset is numbered based on the timepoint t at which it is \r\n",
    "# processed: x_t with t \\in [0 ... N]. We go through the data in linear order.\r\n",
    "# for x_t in X:\r\n",
    "  # if t <= n, then x_t is put into S.\r\n",
    "  # otherwise, x_t becomes a candidate that may replace an item in S:\r\n",
    "  #    generate a random integer r from the interval [0, t]. If r < n, then replace the item S[r] \r\n",
    "  #    with with x_t\r\n",
    "# the probability of an item becoming part of the sample S is n/t and is decreasing with t.\r\n",
    "# the probability of being replaced is higher, the longer an item is part of Sy.\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.utils.random import sample_without_replacement\r\n",
    "from random import randint\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "reservoir = np.array([])\r\n",
    "k = 5\r\n",
    "\r\n",
    "X = np.arange(0, 1000)\r\n",
    "chunk_size = 10\r\n",
    "\r\n",
    "for i in range(0, len(X) // chunk_size):\r\n",
    "  chunk = X[i * chunk_size:(i+1) * chunk_size]\r\n",
    "\r\n",
    "  if i == 0:\r\n",
    "    sample = sample_without_replacement(n_population=chunk_size, n_samples=k, random_state=i)\r\n",
    "    reservoir = chunk[sample]\r\n",
    "  else:\r\n",
    "    candidates = np.array([randint(0, i*chunk_size) for _ in range(chunk_size)])\r\n",
    "    picks = candidates[candidates < k]\r\n",
    "    reservoir[picks] = chunk[picks]\r\n",
    "\r\n",
    "reservoir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Optimized chunk-based Reservoir Sampling\r\n",
    "\r\n",
    "Quote from the reservoir sampling paper:\r\n",
    "> The random variable S(n, t) is defined to be the number of records in the file that are skipped over before the next record is chosen for the reservoir, where n is the size of the sample and where t is the number of records processed so far. For notational simplicity, we shall often write 9 for S(n, t),in which case the parameters n and t will be implicit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.utils.random import sample_without_replacement\r\n",
    "from random import randint\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "reservoir = np.array([])\r\n",
    "k = 5\r\n",
    "\r\n",
    "X = np.arange(0, 1000)\r\n",
    "chunk_size = 10\r\n",
    "\r\n",
    "skip = k\r\n",
    "\r\n",
    "for i in range(0, len(X) // chunk_size):\r\n",
    "  chunk = X[i * chunk_size:(i+1) * chunk_size]\r\n",
    "\r\n",
    "  if i == 0:\r\n",
    "    sample = sample_without_replacement(n_population=chunk_size, n_samples=k, random_state=i)\r\n",
    "    reservoir = chunk[sample]\r\n",
    "    skip = randint(k, (i+1)*chunk_size)\r\n",
    "  elif skip < i*chunk_size:\r\n",
    "    candidate = chunk[skip % chunk_size]\r\n",
    "    reservoir[randint(0, k-1)] = candidate\r\n",
    "    skip = randint(k, i*chunk_size*2)\r\n",
    "\r\n",
    "reservoir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) Progressive Bin-based Sampling with Balanced Classes\r\n",
    "Putting things together."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_bar():\r\n",
    "  # plot the results\r\n",
    "  fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(7, 7), gridspec_kw={\"hspace\": 1.25})\r\n",
    "  ax1.set_title(\"ground truth y values\")\r\n",
    "  ax1.hist(y, **hist_params)\r\n",
    "\r\n",
    "  ax2.set_title(\"sampled y values\")\r\n",
    "  sampled_ys = tuple([y_reservoir[i].reshape(-1, 1) for i in y_reservoir])\r\n",
    "  print([s.shape for s in sampled_ys])\r\n",
    "  sampled_y = np.concatenate(sampled_ys)\r\n",
    "  ax2.hist(sampled_y, **hist_params)\r\n",
    "\r\n",
    "  ax3.set_title(\"doi computed from sampled X\")\r\n",
    "  reservoir = np.concatenate(tuple([X_reservoir[x] for x in X_reservoir]))\r\n",
    "  sampled_X_y = doi.compute_doi(reservoir)\r\n",
    "  ax3.hist(sampled_X_y, **hist_params)\r\n",
    "\r\n",
    "def plot_scatter():\r\n",
    "  reservoir_X = np.concatenate(tuple([X_reservoir[x] for x in X_reservoir]))\r\n",
    "  reservoir_y = doi.compute_doi(reservoir_X)\r\n",
    "  chunk_y = doi.compute_doi(chunk)\r\n",
    "  fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 10))\r\n",
    "  ax1.set_title(\"reservoir\")\r\n",
    "  ax1.scatter(x=reservoir_X[:,0], y=reservoir_X[:,1], c=reservoir_y, alpha=0.1)\r\n",
    "  ax2.set_title(\"chunk\")\r\n",
    "  ax2.scatter(x=chunk[:,0], y=chunk[:,1], c=chunk_y, alpha=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sample_bins(chunk, bins, X_, y_, X_res, y_res, skips, n_bins, k):\r\n",
    "  for j in range(n_bins):\r\n",
    "    if len(bins[bins == j]) == 0:\r\n",
    "      continue\r\n",
    "\r\n",
    "    # find all items in this bin\r\n",
    "    X_in_bin = X_[(bins == j).flatten()]\r\n",
    "    y_in_bin = y_[(bins == j).flatten()]\r\n",
    "\r\n",
    "    # find all items currently in the reservoir for this bin\r\n",
    "    bin_reservoir_X = X_res[j]\r\n",
    "    bin_reservoir_y = y_res[j]\r\n",
    "    skip = skips[j]\r\n",
    "\r\n",
    "    in_bin = len(X_in_bin)\r\n",
    "    in_reservoir = len(bin_reservoir_X)\r\n",
    "\r\n",
    "    # reservoir sample the bin using items assigned to this bin in the current iteration\r\n",
    "    # before replacing, fill the reservoir\r\n",
    "    if chunk == 0 or in_reservoir < k:\r\n",
    "      # either pick all, if there is space, or just pick enough to fill\r\n",
    "      if in_reservoir == 0 and in_bin < k:\r\n",
    "        sample = np.arange(in_bin) # pick all elements in the chunk\r\n",
    "      else:\r\n",
    "        n_samples = np.min((k-in_reservoir, in_bin))\r\n",
    "        sample = sample_without_replacement(n_population=in_bin, n_samples=n_samples, random_state=chunk)\r\n",
    "\r\n",
    "      bin_reservoir_X = np.concatenate((bin_reservoir_X, X_in_bin[sample]))\r\n",
    "      bin_reservoir_y = np.concatenate((bin_reservoir_y.reshape(-1,), y_in_bin[sample]))\r\n",
    "      skip = randint(0, (chunk+1)*chunk_size)\r\n",
    "\r\n",
    "    # replace items in the bin with a probability of 1/N per item throughout the progression\r\n",
    "    else:\r\n",
    "      while skip < chunk*chunk_size:\r\n",
    "        candidate_X = X_in_bin[skip % len(X_in_bin)]\r\n",
    "        candidate_y = y_in_bin[skip % len(X_in_bin)]\r\n",
    "        pick = randint(0, k-1)\r\n",
    "        bin_reservoir_X[pick] = candidate_X\r\n",
    "        bin_reservoir_y[pick] = candidate_y\r\n",
    "        skip = randint(0, (chunk+1)*chunk_size)\r\n",
    "    \r\n",
    "    skips[j] = skip\r\n",
    "    X_res[j] = bin_reservoir_X\r\n",
    "    y_res[j] = bin_reservoir_y\r\n",
    "\r\n",
    "  return X_reservoir, y_reservoir\r\n",
    "\r\n",
    "def rebin_reservoirs(chunk, X_res, y_res, skips, n_bins, k):\r\n",
    "  '''\r\n",
    "  Recompute the doi function over all items in the reservoir, then redistribute the data into bins \r\n",
    "  again.\r\n",
    "  '''\r\n",
    "  X_ = np.concatenate(tuple([X_reservoir[x] for x in X_reservoir]))\r\n",
    "  y_ = doi.compute_doi(X_)\r\n",
    "  bins = get_bins(y_)\r\n",
    "\r\n",
    "  X_res = {}\r\n",
    "  y_res = {}\r\n",
    "  for i in range(n_bins):\r\n",
    "    X_res[i] = np.empty((0, X.shape[1]))\r\n",
    "    y_res[i] = np.empty((0, 1))\r\n",
    "\r\n",
    "  return sample_bins(chunk, bins, X_, y_, X_res, y_res, skips, n_bins, k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_bins(y: np.ndarray, n_bins=5):\r\n",
    "  '''\r\n",
    "  Sorts the input by value and then divides it into bin_size bins of equal size. If len(y) / n_bins \r\n",
    "  has a residual, items assigned to the \"overflow\" bin are merged into the last one to guarantee\r\n",
    "  n_bins.\r\n",
    "  '''\r\n",
    "  sort = np.argsort(y.flatten())\r\n",
    "  total_size = len(y)\r\n",
    "  bin_size = total_size // n_bins\r\n",
    "  index = np.arange(0, total_size)\r\n",
    "  index[sort] = index // bin_size\r\n",
    "\r\n",
    "  # merge \"residual\" bin with last bin\r\n",
    "  if total_size // n_bins != total_size / n_bins:\r\n",
    "    index[index == n_bins] = n_bins - 1\r\n",
    "\r\n",
    "  return index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from random import randint\r\n",
    "import numpy as np\r\n",
    "from random import random\r\n",
    "from sklearn.datasets import make_blobs\r\n",
    "from sklearn.preprocessing import KBinsDiscretizer\r\n",
    "from sklearn.utils.random import sample_without_replacement\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "N = 10000\r\n",
    "features = 2\r\n",
    "chunk_size = 1000\r\n",
    "\r\n",
    "chunks = N // chunk_size\r\n",
    "k = chunk_size\r\n",
    "n_bins = 5\r\n",
    "\r\n",
    "blobs_params = {\"n_samples\": N, \"n_features\": features}\r\n",
    "hist_params = {\"alpha\": 0.73, \"ec\": \"#000\"}\r\n",
    "binning = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"kmeans\")\r\n",
    "\r\n",
    "X = make_blobs(centers=6, cluster_std=1, **blobs_params)[0]\r\n",
    "y = doi.compute_doi(X)\r\n",
    "\r\n",
    "# create empty reservoirs for the progressive sample per bin. Each bin reservoir contains teh \r\n",
    "X_reservoir: dict[str, list] = {} # progressively stores data\r\n",
    "y_reservoir: dict[str, float] = {} # progressively stores doi\r\n",
    "\r\n",
    "# for each bin, create an empty reservoir\r\n",
    "for i in range(n_bins):\r\n",
    "  X_reservoir[i] = np.empty((0, X.shape[1]))\r\n",
    "  y_reservoir[i] = np.empty((0, 1))\r\n",
    "\r\n",
    "skips = [k for _ in range(n_bins)]\r\n",
    "\r\n",
    "# last iteration's per-bin mean values\r\n",
    "previous_bin_means = np.array([])\r\n",
    "\r\n",
    "# sum of squared differences betwen partial doi and full doi computation\r\n",
    "mse = []\r\n",
    "\r\n",
    "for i in range(chunks):\r\n",
    "  print(\"\\n#chunk:\", i)\r\n",
    "\r\n",
    "  chunk = X[i*chunk_size:(i+1)*chunk_size]\r\n",
    "\r\n",
    "  # compute the doi function over known items and the items in the new chunk\r\n",
    "  reservoir = np.concatenate(tuple([X_reservoir[x] for x in X_reservoir]))\r\n",
    "  X_ = np.append(reservoir, chunk, axis=0)\r\n",
    "  y_ = doi.compute_doi(X_)\r\n",
    "\r\n",
    "  # item -> bin (bins can be used to index X_ and y_)\r\n",
    "  bins = get_bins(y_)\r\n",
    "\r\n",
    "  # fill the reservoirs by drawing random samples from the data\r\n",
    "  X_reservoir, y_reservoir = sample_bins(i, bins, X_, y_, X_reservoir, y_reservoir, skips, n_bins, k)\r\n",
    "\r\n",
    "  # TODO: check if rebinning is necessary\r\n",
    "  # compute the prediction error for each chunk\r\n",
    "  error = np.mean((y_[-chunk_size:]-y[i*chunk_size:(i+1)*chunk_size])**2)\r\n",
    "  mse += [error]\r\n",
    "\r\n",
    "  # fixed rebin every 4 iterations\r\n",
    "  if i > 0 and i % 4 == 0:\r\n",
    "    print(\"rebinning\")\r\n",
    "    X_reservoir, y_reservoir = rebin_reservoirs(i, X_reservoir, y_reservoir, skips, n_bins, k)\r\n",
    "  \r\n",
    "  plot_scatter()\r\n",
    "\r\n",
    "print(\"error progression:\", mse)\r\n",
    "plot_bar()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}