{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Progressively Updating DOI values\r\n",
    "Another challenge when computing doi functions in chunks over large data is that the computed values \"age\".\r\n",
    "The more data we process, the more representative of the entire dataset our predictions become, which means that values computed early on may no longer be valid if they are recomputed after hundreds of iterations.\r\n",
    "In turn, we need a mechanism to update outdated doi values.\r\n",
    "\r\n",
    "These updates can happen in different ways:\r\n",
    "0. No update: User interest lies on the latest data, old interest values \"decay\" until they become 0.\r\n",
    "1. In regular intervals, compute the doi of old items with the next chunk.\r\n",
    "2. Whenever some metric exceeds a threshold, compute the doi of old items with the next chunk.\r\n",
    "3. Predict the interest of \"old\" items based on latest doi information (train a regression model, or use fx. interpolation between knn, or use buckets in view space quadtree)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the DOI function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sys import path;path.append(\"../\")\r\n",
    "from outlierness_component import OutliernessComponent\r\n",
    "\r\n",
    "outlierness = OutliernessComponent()\r\n",
    "\r\n",
    "def doi(X: np.ndarray):\r\n",
    "  df = pd.DataFrame(X)\r\n",
    "  return outlierness.compute_doi(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import make_blobs\r\n",
    "from progressive_bin_sampler import ProgressiveBinSampler\r\n",
    "\r\n",
    "N = 10000\r\n",
    "features = 2\r\n",
    "chunk_size = 1000\r\n",
    "\r\n",
    "chunks = N // chunk_size\r\n",
    "\r\n",
    "blobs_params = {\"n_samples\": N, \"n_features\": features}\r\n",
    "\r\n",
    "X = make_blobs(centers=6, cluster_std=1, **blobs_params)[0]\r\n",
    "y = doi(X)\r\n",
    "\r\n",
    "sampler = ProgressiveBinSampler(n_dims=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Progressive Update Strategies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No Update: Decaying Interest Values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doi_value_chunk = np.array((0, )) # stores the chunk at which the data was first processed\r\n",
    "old_doi_values = np.array((0, )) # stores the original doi value from when the chunk was processed\r\n",
    "updated_doi_values = np.array((0, )) # stores the doi values after the update\r\n",
    "\r\n",
    "sampler.reset_reservoirs()\r\n",
    "\r\n",
    "for i in range(chunks):\r\n",
    "  chunk = X[i*chunk_size:(i+1)*chunk_size]\r\n",
    "  progressive_sample = sampler.get_current_sample()\r\n",
    "  X_ = np.append(progressive_sample, chunk, axis=0)\r\n",
    "  y_ = doi(X_)\r\n",
    "\r\n",
    "  new_doi_values = y_[-chunk_size:]\r\n",
    "\r\n",
    "  sampler.add_chunk(chunk, new_doi_values, i*chunk_size)\r\n",
    "  old_doi_values = np.append(old_doi_values, new_doi_values)\r\n",
    "  age = np.full_like(new_doi_values, fill_value=i)\r\n",
    "  doi_value_chunk = np.append(doi_value_chunk, age)\r\n",
    "\r\n",
    "  # \"radioactive\" decay, where the newest values remain the same\r\n",
    "  updated_doi_values = old_doi_values * (0.5**(i - doi_value_chunk))\r\n",
    "\r\n",
    "[(j, updated_doi_values[doi_value_chunk == j].mean()) for j in range(chunks)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recomputation: Recompute Interest Values\r\n",
    "Mix the \"old\" data into the computation of new values based on some decision rule."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Recompute in regular Intervals\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doi_value_chunk = np.array((0, )) # stores the chunk at which the data was seen first\r\n",
    "updated_doi_values = np.array((0, )) # stores the doi values after the update\r\n",
    "\r\n",
    "max_age = 2 # update the doi of each chunk every max_age iterations\r\n",
    "\r\n",
    "sampler.reset_reservoirs()\r\n",
    "\r\n",
    "for i in range(chunks):\r\n",
    "  chunk = X[i*chunk_size:(i+1)*chunk_size]\r\n",
    "  progressive_sample = sampler.get_current_sample()\r\n",
    "  sample_size = len(progressive_sample)\r\n",
    "  X_ = np.append(progressive_sample, chunk, axis=0)\r\n",
    "\r\n",
    "  # add all chunks that haven't been updated in the last max_age iterations:\r\n",
    "  added_index = {}\r\n",
    "  for j in range(i):\r\n",
    "    if j % max_age == 0:\r\n",
    "      added_index[j] = len(X_) # store the beginning index for this chunk to find it later\r\n",
    "      X_ = np.append(X_, X[j*chunk_size:(j+1)*chunk_size], axis=0)\r\n",
    "\r\n",
    "  y_ = doi(X_)\r\n",
    "\r\n",
    "  new_doi_values = y_[sample_size:sample_size + chunk_size]\r\n",
    "\r\n",
    "  sampler.add_chunk(chunk, new_doi_values, i*chunk_size)\r\n",
    "  updated_doi_values = np.append(updated_doi_values, new_doi_values)\r\n",
    "\r\n",
    "  age = np.full_like(new_doi_values, fill_value=i)\r\n",
    "  doi_value_chunk = np.append(doi_value_chunk, age)\r\n",
    "\r\n",
    "  for j in range(1, i):\r\n",
    "    if j % max_age == 0:\r\n",
    "      index = added_index[j]\r\n",
    "      updated_doi_values[doi_value_chunk == j] = y_[index:index+chunk_size]\r\n",
    "\r\n",
    "[(j, updated_doi_values[doi_value_chunk == j].mean()) for j in range(chunks)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Recompute based on a Bin-based Metric"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doi_value_chunk = np.array((0, )) # stores the chunk at which the data was seen first\r\n",
    "updated_doi_values = np.array((0, )) # stores the doi values after the update\r\n",
    "\r\n",
    "sampler.reset_reservoirs()\r\n",
    "\r\n",
    "for i in range(chunks):\r\n",
    "  chunk = X[i*chunk_size:(i+1)*chunk_size]\r\n",
    "  progressive_sample = sampler.get_current_sample()\r\n",
    "  X_ = np.append(progressive_sample, chunk, axis=0)\r\n",
    "  y_ = doi(X_)\r\n",
    "\r\n",
    "  # TODO: compute the per-bin mean\r\n",
    "  # TODO: detect mean shift per bin\r\n",
    "  # TODO: find all items that belong to this bin and add them to the recomputation\r\n",
    "  # TODO: update the doi of all items that were affected by that metric\r\n",
    "\r\n",
    "  new_doi_values = y_[-chunk_size:]\r\n",
    "\r\n",
    "  sampler.add_chunk(chunk, new_doi_values, i*chunk_size)\r\n",
    "  age = np.full_like(new_doi_values, fill_value=i)\r\n",
    "  doi_value_chunk = np.append(doi_value_chunk, age)\r\n",
    "  updated_doi_values = np.append(updated_doi_values, new_doi_values)\r\n",
    "\r\n",
    "[(j, updated_doi_values[doi_value_chunk == j].mean()) for j in range(chunks)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction-based: Estimate previous Interest Values from latest Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Option 1: Use nearest neighbor in KDTree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KDTree\r\n",
    "\r\n",
    "doi_value_chunk = np.array((0, )) # stores the chunk at which the data was seen first\r\n",
    "updated_doi_values = np.array((0, )) # stores the doi values after the update\r\n",
    "\r\n",
    "sampler.reset_reservoirs()\r\n",
    "\r\n",
    "for i in range(chunks):\r\n",
    "  chunk = X[i*chunk_size:(i+1)*chunk_size]\r\n",
    "  progressive_sample = sampler.get_current_sample()\r\n",
    "  X_ = np.append(progressive_sample, chunk, axis=0)\r\n",
    "  y_ = doi(X_)\r\n",
    "\r\n",
    "  new_doi_values = y_[-chunk_size:]\r\n",
    "  updated_doi_values = np.append(updated_doi_values, new_doi_values)\r\n",
    "\r\n",
    "  kdtree = KDTree(X_)\r\n",
    "\r\n",
    "  if i > 0:\r\n",
    "    # find knn in chunk for all points not in chunk\r\n",
    "    knn = kdtree.query(X[:i*chunk_size], return_distance=False).reshape(-1, )\r\n",
    "    updated_doi_values[:i*chunk_size] = y_[knn] # return type of query is odd\r\n",
    "\r\n",
    "  # reservoir sample all new doi values\r\n",
    "  sampler.add_chunk(chunk, new_doi_values, i*chunk_size)\r\n",
    "  age = np.full_like(new_doi_values, fill_value=i)\r\n",
    "  doi_value_chunk = np.append(doi_value_chunk, age)\r\n",
    "\r\n",
    "[(j, updated_doi_values[doi_value_chunk == j].mean()) for j in range(chunks)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Option 2: Predict from sampled items"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "\r\n",
    "doi_value_chunk = np.array((0, )) # stores the chunk at which the data was seen first\r\n",
    "updated_doi_values = np.array((0, )) # stores the doi values after the update\r\n",
    "\r\n",
    "sampler.reset_reservoirs()\r\n",
    "tree = DecisionTreeRegressor()\r\n",
    "\r\n",
    "for i in range(chunks):\r\n",
    "  chunk = X[i*chunk_size:(i+1)*chunk_size]\r\n",
    "  y_ = doi(chunk)\r\n",
    "\r\n",
    "  if i > 0:\r\n",
    "    X_sample, y_sample = sampler.get_current_sample(return_labels=True)\r\n",
    "    tree.fit(X_sample, y_sample)\r\n",
    "    y2 = tree.predict(chunk)\r\n",
    "    y_ = np.mean([y_, y2], axis=0)\r\n",
    "\r\n",
    "  new_doi_values = y_\r\n",
    "  updated_doi_values = np.append(updated_doi_values, new_doi_values)\r\n",
    "\r\n",
    "  # reservoir sample all new doi values\r\n",
    "  sampler.add_chunk(chunk, new_doi_values, i*chunk_size)\r\n",
    "  age = np.full_like(new_doi_values, fill_value=i)\r\n",
    "  doi_value_chunk = np.append(doi_value_chunk, age)\r\n",
    "\r\n",
    "[(j, updated_doi_values[doi_value_chunk == j].mean()) for j in range(chunks)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}